from flask import Flask, render_template, request, jsonify, send_file
from openai import OpenAI
from dotenv import load_dotenv
import os
import json
from datetime import datetime
from striprtf.striprtf import rtf_to_text
import pdfplumber
from docx import Document
import openpyxl
import mysql.connector
from mysql.connector import Error
import shutil

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ
load_dotenv()

app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max
app.config['UPLOAD_FOLDER'] = 'uploaded_files'



# ØªÙ†Ø¸ÛŒÙ… OpenAI
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª MySQL
DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'user': os.getenv('DB_USER', 'root'),
    'password': os.getenv('DB_PASSWORD', ''),
    'database': os.getenv('DB_NAME', 'crm_analyzer'),
    'charset': 'utf8mb4'
}

def get_db_connection():
    """Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
    try:
        conn = mysql.connector.connect(**DB_CONFIG)
        return conn
    except Error as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {e}")
        return None

def save_analysis_to_db(file_info, analysis_data):
    """Ø°Ø®ÛŒØ±Ù‡ ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
    conn = get_db_connection()
    if not conn:
        return None
    
    try:
        cursor = conn.cursor()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        nums = analysis_data.get('ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ_Ø¹Ø¯Ø¯ÛŒ', {})
        text = analysis_data.get('ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ_Ù…ØªÙ†ÛŒ', {})
        lists = analysis_data.get('Ù„ÛŒØ³Øª_Ù‡Ø§', {})
        stats = analysis_data.get('Ø¢Ù…Ø§Ø±', {})
        best = analysis_data.get('Ø¨Ù‡ØªØ±ÛŒÙ†_Ù‡Ø§', {})
        reasons_dec = analysis_data.get('Ø¯Ù„Ø§ÛŒÙ„_Ú©Ø§Ù‡Ø´_Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§', {})
        reasons_inc = analysis_data.get('Ø¯Ù„Ø§ÛŒÙ„_Ú©Ø³Ø¨_Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§', {})
        
        # Insert ØªØ­Ù„ÛŒÙ„ Ø§ØµÙ„ÛŒ
        query = """
        INSERT INTO analyses (
            file_name, file_path, file_size, file_type, analyzed_at,
            score_total, score_rapport, score_needs, score_value, 
            score_objection, score_price, score_closing, score_followup,
            score_empathy, score_listening,
            lead_quality_percent, open_questions_count, objections_count,
            objection_success_percent, closing_attempts_count, customer_feeling_score,
            closing_readiness_percent, seller_technical_density_percent,
            customer_technical_density_percent, customer_price_sensitivity_percent,
            customer_risk_sensitivity_percent, customer_time_sensitivity_percent,
            yes_ladder_count,
            disc_d, disc_i, disc_s, disc_c,
            seller_name, seller_code, customer_name, call_duration,
            call_direction, call_stage, call_warmth, call_nature,
            product, seller_level, disc_type, disc_evidence, disc_interaction_guide,
            preferred_channel, customer_awareness_level,
            customer_talk_ratio, seller_talk_ratio,
            summary, customer_personality_analysis, seller_individual_performance,
            call_type_readiness, next_action,
            rapport_decrease_reasons, needs_decrease_reasons, value_decrease_reasons,
            objection_decrease_reasons, price_decrease_reasons, closing_decrease_reasons,
            followup_decrease_reasons, empathy_decrease_reasons, listening_decrease_reasons,
            rapport_increase_reasons, needs_increase_reasons, value_increase_reasons,
            objection_increase_reasons, price_increase_reasons, closing_increase_reasons,
            followup_increase_reasons, empathy_increase_reasons, listening_increase_reasons,
            total_calls, successful_calls, no_answer_calls, referred_calls,
            best_seller, best_seller_reason, best_customer, best_customer_reason,
            full_analysis
        ) VALUES (
            %s, %s, %s, %s, %s,
            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
            %s, %s, %s, %s,
            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
            %s, %s,
            %s, %s, %s, %s, %s,
            %s, %s, %s, %s, %s, %s, %s, %s, %s,
            %s, %s, %s, %s, %s, %s, %s, %s, %s,
            %s, %s, %s, %s,
            %s, %s, %s, %s,
            %s
        )
        """
        
        values = (
            # ÙØ§ÛŒÙ„
            file_info['name'], file_info['path'], file_info['size'], file_info['type'],
            datetime.now(),
            # Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§
            nums.get('Ø§Ù…ØªÛŒØ§Ø²_Ú©Ù„', 0),
            nums.get('Ø§Ù…ØªÛŒØ§Ø²_Ø¨Ø±Ù‚Ø±Ø§Ø±ÛŒ_Ø§Ø±ØªØ¨Ø§Ø·', 0),
            nums.get('Ø§Ù…ØªÛŒØ§Ø²_Ù†ÛŒØ§Ø²Ø³Ù†Ø¬ÛŒ', 0),
            nums.get('Ø§Ù…ØªÛŒØ§Ø²_Ø§Ø±Ø²Ø´_ÙØ±ÙˆØ´ÛŒ', 0),
            nums.get('Ø§Ù…ØªÛŒØ§Ø²_Ù…Ø¯ÛŒØ±ÛŒØª_Ø§Ø¹ØªØ±Ø§Ø¶', 0),
            nums.get('Ø§Ù…ØªÛŒØ§Ø²_Ø´ÙØ§ÙÛŒØª_Ù‚ÛŒÙ…Øª', 0),
            nums.get('Ø§Ù…ØªÛŒØ§Ø²_Ø¨Ø³ØªÙ†_ÙØ±ÙˆØ´', 0),
            nums.get('Ø§Ù…ØªÛŒØ§Ø²_Ù¾ÛŒÚ¯ÛŒØ±ÛŒ', 0),
            nums.get('Ø§Ù…ØªÛŒØ§Ø²_Ù‡Ù…Ø³ÙˆÛŒÛŒ_Ø§Ø­Ø³Ø§Ø³ÛŒ', 0),
            nums.get('Ø§Ù…ØªÛŒØ§Ø²_Ø´Ù†ÙˆÙ†Ø¯Ú¯ÛŒ', 0),
            # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø§Ø¶Ø§ÙÛŒ
            nums.get('Ú©ÛŒÙÛŒØª_Ù„ÛŒØ¯_Ø¯Ø±ØµØ¯', 0),
            nums.get('ØªØ¹Ø¯Ø§Ø¯_Ø³ÙˆØ§Ù„Ø§Øª_Ø¨Ø§Ø²', 0),
            nums.get('ØªØ¹Ø¯Ø§Ø¯_Ø§Ø¹ØªØ±Ø§Ø¶', 0),
            nums.get('Ø¯Ø±ØµØ¯_Ù¾Ø§Ø³Ø®_Ù…ÙˆÙÙ‚_Ø¨Ù‡_Ø§Ø¹ØªØ±Ø§Ø¶', 0),
            nums.get('ØªØ¹Ø¯Ø§Ø¯_ØªÙ„Ø§Ø´_Ø¨Ø±Ø§ÛŒ_Ø¨Ø³ØªÙ†', 0),
            nums.get('Ø§Ù…ØªÛŒØ§Ø²_Ø§Ø­Ø³Ø§Ø³_Ù…Ø´ØªØ±ÛŒ', 0),
            nums.get('Ø¢Ù…Ø§Ø¯Ú¯ÛŒ_Ø¨Ø³ØªÙ†_Ø¯Ø±ØµØ¯', 0),
            nums.get('Ú†Ú¯Ø§Ù„ÛŒ_Ø§Ø·Ù„Ø§Ø¹Ø§Øª_ÙÙ†ÛŒ_ÙØ±ÙˆØ´Ù†Ø¯Ù‡_Ø¯Ø±ØµØ¯', 0),
            nums.get('Ú†Ú¯Ø§Ù„ÛŒ_Ø§Ø·Ù„Ø§Ø¹Ø§Øª_ÙÙ†ÛŒ_Ù…Ø´ØªØ±ÛŒ_Ø¯Ø±ØµØ¯', 0),
            nums.get('Ø­Ø³Ø§Ø³ÛŒØª_Ù‚ÛŒÙ…Øª_Ù…Ø´ØªØ±ÛŒ_Ø¯Ø±ØµØ¯', 0),
            nums.get('Ø­Ø³Ø§Ø³ÛŒØª_Ø±ÛŒØ³Ú©_Ù…Ø´ØªØ±ÛŒ_Ø¯Ø±ØµØ¯', 0),
            nums.get('Ø­Ø³Ø§Ø³ÛŒØª_Ø²Ù…Ø§Ù†_Ù…Ø´ØªØ±ÛŒ_Ø¯Ø±ØµØ¯', 0),
            nums.get('ØªØ¹Ø¯Ø§Ø¯_Ø¨Ù„Ù‡_Ù¾Ù„Ù‡_Ø§ÛŒ', 0),
            # DISC
            nums.get('disc_d', 0),
            nums.get('disc_i', 0),
            nums.get('disc_s', 0),
            nums.get('disc_c', 0),
            # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ØªÙ†ÛŒ
            text.get('Ù†Ø§Ù…_ÙØ±ÙˆØ´Ù†Ø¯Ù‡'),
            text.get('Ú©Ø¯_ÙØ±ÙˆØ´Ù†Ø¯Ù‡'),
            text.get('Ù†Ø§Ù…_Ù…Ø´ØªØ±ÛŒ'),
            text.get('Ù…Ø¯Øª_ØªÙ…Ø§Ø³'),
            text.get('Ù†ÙˆØ¹_ØªÙ…Ø§Ø³_Ø¬Ù‡Øª'),
            text.get('Ù†ÙˆØ¹_ØªÙ…Ø§Ø³_Ù…Ø±Ø­Ù„Ù‡'),
            text.get('Ù†ÙˆØ¹_ØªÙ…Ø§Ø³_Ú¯Ø±Ù…ÛŒ'),
            text.get('Ù†ÙˆØ¹_ØªÙ…Ø§Ø³_Ù…Ø§Ù‡ÛŒØª'),
            text.get('Ù…Ø­ØµÙˆÙ„'),
            text.get('Ø³Ø·Ø­_ÙØ±ÙˆØ´Ù†Ø¯Ù‡'),
            text.get('disc_ØªÛŒÙ¾'),
            json.dumps(text.get('disc_Ø´ÙˆØ§Ù‡Ø¯', []), ensure_ascii=False),
            text.get('disc_Ø±Ø§Ù‡Ù†Ù…Ø§'),
            text.get('ØªØ±Ø¬ÛŒØ­_Ú©Ø§Ù†Ø§Ù„'),
            text.get('Ø³Ø·Ø­_Ø¢Ú¯Ø§Ù‡ÛŒ_Ù…Ø´ØªØ±ÛŒ'),
            text.get('Ù†Ø³Ø¨Øª_Ø²Ù…Ø§Ù†_ØµØ­Ø¨Øª_Ù…Ø´ØªØ±ÛŒ_Ø¨Ù‡_ÙØ±ÙˆØ´Ù†Ø¯Ù‡'),
            text.get('Ù†Ø³Ø¨Øª_Ø²Ù…Ø§Ù†_ØµØ­Ø¨Øª_ÙØ±ÙˆØ´Ù†Ø¯Ù‡_Ø¨Ù‡_Ù…Ø´ØªØ±ÛŒ'),
            text.get('Ø®Ù„Ø§ØµÙ‡'),
            text.get('ØªØ­Ù„ÛŒÙ„_Ø´Ø®ØµÛŒØª_Ù…Ø´ØªØ±ÛŒ'),
            text.get('Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ_Ø¹Ù…Ù„Ú©Ø±Ø¯_ÙØ±Ø¯ÛŒ_ÙØ±ÙˆØ´Ù†Ø¯Ù‡'),
            text.get('ØªØ´Ø®ÛŒØµ_Ø¢Ù…Ø§Ø¯Ú¯ÛŒ'),
            text.get('Ø§Ù‚Ø¯Ø§Ù…_Ø¨Ø¹Ø¯ÛŒ'),
            # Ø¯Ù„Ø§ÛŒÙ„ Ú©Ø§Ù‡Ø´
            json.dumps(reasons_dec.get('Ø¨Ø±Ù‚Ø±Ø§Ø±ÛŒ_Ø§Ø±ØªØ¨Ø§Ø·', []), ensure_ascii=False),
            json.dumps(reasons_dec.get('Ù†ÛŒØ§Ø²Ø³Ù†Ø¬ÛŒ', []), ensure_ascii=False),
            json.dumps(reasons_dec.get('Ø§Ø±Ø²Ø´_ÙØ±ÙˆØ´ÛŒ', []), ensure_ascii=False),
            json.dumps(reasons_dec.get('Ù…Ø¯ÛŒØ±ÛŒØª_Ø§Ø¹ØªØ±Ø§Ø¶', []), ensure_ascii=False),
            json.dumps(reasons_dec.get('Ø´ÙØ§ÙÛŒØª_Ù‚ÛŒÙ…Øª', []), ensure_ascii=False),
            json.dumps(reasons_dec.get('Ø¨Ø³ØªÙ†_ÙØ±ÙˆØ´', []), ensure_ascii=False),
            json.dumps(reasons_dec.get('Ù¾ÛŒÚ¯ÛŒØ±ÛŒ', []), ensure_ascii=False),
            json.dumps(reasons_dec.get('Ù‡Ù…Ø³ÙˆÛŒÛŒ_Ø§Ø­Ø³Ø§Ø³ÛŒ', []), ensure_ascii=False),
            json.dumps(reasons_dec.get('Ø´Ù†ÙˆÙ†Ø¯Ú¯ÛŒ', []), ensure_ascii=False),
            # Ø¯Ù„Ø§ÛŒÙ„ Ú©Ø³Ø¨
            json.dumps(reasons_inc.get('Ø¨Ø±Ù‚Ø±Ø§Ø±ÛŒ_Ø§Ø±ØªØ¨Ø§Ø·', []), ensure_ascii=False),
            json.dumps(reasons_inc.get('Ù†ÛŒØ§Ø²Ø³Ù†Ø¬ÛŒ', []), ensure_ascii=False),
            json.dumps(reasons_inc.get('Ø§Ø±Ø²Ø´_ÙØ±ÙˆØ´ÛŒ', []), ensure_ascii=False),
            json.dumps(reasons_inc.get('Ù…Ø¯ÛŒØ±ÛŒØª_Ø§Ø¹ØªØ±Ø§Ø¶', []), ensure_ascii=False),
            json.dumps(reasons_inc.get('Ø´ÙØ§ÙÛŒØª_Ù‚ÛŒÙ…Øª', []), ensure_ascii=False),
            json.dumps(reasons_inc.get('Ø¨Ø³ØªÙ†_ÙØ±ÙˆØ´', []), ensure_ascii=False),
            json.dumps(reasons_inc.get('Ù¾ÛŒÚ¯ÛŒØ±ÛŒ', []), ensure_ascii=False),
            json.dumps(reasons_inc.get('Ù‡Ù…Ø³ÙˆÛŒÛŒ_Ø§Ø­Ø³Ø§Ø³ÛŒ', []), ensure_ascii=False),
            json.dumps(reasons_inc.get('Ø´Ù†ÙˆÙ†Ø¯Ú¯ÛŒ', []), ensure_ascii=False),
            # Ø¢Ù…Ø§Ø±
            stats.get('ØªØ¹Ø¯Ø§Ø¯_Ú©Ù„_ØªÙ…Ø§Ø³_Ù‡Ø§', 0),
            stats.get('ØªÙ…Ø§Ø³_Ù‡Ø§ÛŒ_Ù…ÙˆÙÙ‚', 0),
            stats.get('ØªÙ…Ø§Ø³_Ù‡Ø§ÛŒ_Ø¨ÛŒ_Ù¾Ø§Ø³Ø®', 0),
            stats.get('ØªÙ…Ø§Ø³_Ù‡Ø§ÛŒ_Ø§Ø±Ø¬Ø§Ø¹ÛŒ', 0),
            # Ø¨Ù‡ØªØ±ÛŒÙ†â€ŒÙ‡Ø§
            best.get('Ø¨Ù‡ØªØ±ÛŒÙ†_ÙØ±ÙˆØ´Ù†Ø¯Ù‡', {}).get('Ù†Ø§Ù…'),
            best.get('Ø¨Ù‡ØªØ±ÛŒÙ†_ÙØ±ÙˆØ´Ù†Ø¯Ù‡', {}).get('Ø¯Ù„ÛŒÙ„'),
            best.get('Ø¨Ù‡ØªØ±ÛŒÙ†_Ù…Ø´ØªØ±ÛŒ', {}).get('Ù†Ø§Ù…'),
            best.get('Ø¨Ù‡ØªØ±ÛŒÙ†_Ù…Ø´ØªØ±ÛŒ', {}).get('Ø¯Ù„ÛŒÙ„'),
            # JSON Ú©Ø§Ù…Ù„
            json.dumps(analysis_data, ensure_ascii=False)
        )
        
        cursor.execute(query, values)
        analysis_id = cursor.lastrowid
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† ÙØ¹Ø§Ù„
        users = stats.get('Ú©Ø§Ø±Ø¨Ø±Ø§Ù†_ÙØ¹Ø§Ù„', [])
        for user in users:
            if isinstance(user, dict):
                cursor.execute(
                    "INSERT INTO active_users (analysis_id, user_name, call_count, performance_note) VALUES (%s, %s, %s, %s)",
                    (analysis_id, user.get('Ù†Ø§Ù…'), user.get('ØªØ¹Ø¯Ø§Ø¯_ØªÙ…Ø§Ø³', 1), user.get('ÛŒØ§Ø¯Ø¯Ø§Ø´Øª_Ø¹Ù…Ù„Ú©Ø±Ø¯'))
                )
            else:
                cursor.execute(
                    "INSERT INTO active_users (analysis_id, user_name) VALUES (%s, %s)",
                    (analysis_id, user)
                )
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø´ØªØ±ÛŒØ§Ù†
        customers = stats.get('Ù…Ø´ØªØ±ÛŒØ§Ù†_Ù¾Ø±ØªÙ…Ø§Ø³', [])
        for customer in customers:
            if isinstance(customer, dict):
                cursor.execute(
                    "INSERT INTO top_customers (analysis_id, customer_name, contact_count, interaction_quality) VALUES (%s, %s, %s, %s)",
                    (analysis_id, customer.get('Ù†Ø§Ù…'), customer.get('ØªØ¹Ø¯Ø§Ø¯_ØªÙ…Ø§Ø³', 1), customer.get('Ú©ÛŒÙÛŒØª_ØªØ¹Ø§Ù…Ù„'))
                )
            else:
                cursor.execute(
                    "INSERT INTO top_customers (analysis_id, customer_name) VALUES (%s, %s)",
                    (analysis_id, customer)
                )
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†Ù‚Ø§Ø· Ù‚ÙˆØª
        for strength in lists.get('Ù†Ù‚Ø§Ø·_Ù‚ÙˆØª', []):
            cursor.execute(
                "INSERT INTO strengths (analysis_id, strength) VALUES (%s, %s)",
                (analysis_id, strength)
            )
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù
        for weakness in lists.get('Ù†Ù‚Ø§Ø·_Ø¶Ø¹Ù', []):
            cursor.execute(
                "INSERT INTO weaknesses (analysis_id, weakness) VALUES (%s, %s)",
                (analysis_id, weakness)
            )
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø§Ø¹ØªØ±Ø§Ø¶Ø§Øª
        for objection in lists.get('Ø§Ø¹ØªØ±Ø§Ø¶Ø§Øª', []):
            cursor.execute(
                "INSERT INTO objections (analysis_id, objection) VALUES (%s, %s)",
                (analysis_id, objection)
            )
        
        # Ø°Ø®ÛŒØ±Ù‡ ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§
        for technique in lists.get('ØªÚ©Ù†ÛŒÚ©Ù‡Ø§', []):
            cursor.execute(
                "INSERT INTO techniques (analysis_id, technique) VALUES (%s, %s)",
                (analysis_id, technique)
            )
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú©Ù„Ù…Ø§Øª Ù…Ø«Ø¨Øª
        for keyword in lists.get('Ú©Ù„Ù…Ø§Øª_Ù…Ø«Ø¨Øª', []):
            cursor.execute(
                "INSERT INTO positive_keywords (analysis_id, keyword) VALUES (%s, %s)",
                (analysis_id, keyword)
            )
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú©Ù„Ù…Ø§Øª Ù…Ù†ÙÛŒ
        for keyword in lists.get('Ú©Ù„Ù…Ø§Øª_Ù…Ù†ÙÛŒ', []):
            cursor.execute(
                "INSERT INTO negative_keywords (analysis_id, keyword) VALUES (%s, %s)",
                (analysis_id, keyword)
            )
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø±ÛŒØ³Ú©â€ŒÙ‡Ø§
        for risk in lists.get('Ø±ÛŒØ³Ú©_Ù‡Ø§', []):
            cursor.execute(
                "INSERT INTO risks (analysis_id, risk) VALUES (%s, %s)",
                (analysis_id, risk)
            )
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø±Ø¹Ø§ÛŒØª Ù†Ø´Ø¯Ù‡
        for param in lists.get('Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ_Ø±Ø¹Ø§ÛŒØª_Ù†Ø´Ø¯Ù‡', []):
            cursor.execute(
                "INSERT INTO missed_parameters (analysis_id, parameter) VALUES (%s, %s)",
                (analysis_id, param)
            )
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø§Ø´ØªØ¨Ø§Ù‡Ø§Øª
        for mistake in lists.get('Ø§Ø´ØªØ¨Ø§Ù‡Ø§Øª_Ø±Ø§ÛŒØ¬', []):
            cursor.execute(
                "INSERT INTO common_mistakes (analysis_id, mistake) VALUES (%s, %s)",
                (analysis_id, mistake)
            )
        
        conn.commit()
        print(f"âœ… ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ ID {analysis_id} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        return analysis_id
        
    except Error as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡: {e}")
        conn.rollback()
        return None
    finally:
        cursor.close()
        conn.close()

def get_all_analyses():
    """Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª ØªÙ…Ø§Ù… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§"""
    conn = get_db_connection()
    if not conn:
        return []
    
    try:
        cursor = conn.cursor(dictionary=True)
        query = """
        SELECT 
            id, file_name, created_at, analyzed_at,
            score_total, seller_name, customer_name, product,
            total_calls, successful_calls
        FROM analyses
        ORDER BY created_at DESC
        """
        cursor.execute(query)
        results = cursor.fetchall()
        
        # ØªØ¨Ø¯ÛŒÙ„ datetime Ø¨Ù‡ string
        for row in results:
            row['created_at'] = row['created_at'].isoformat() if row['created_at'] else None
            row['analyzed_at'] = row['analyzed_at'].isoformat() if row['analyzed_at'] else None
        
        return results
    except Error as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª: {e}")
        return []
    finally:
        cursor.close()
        conn.close()

def get_analysis_by_id(analysis_id):
    """Ø¯Ø±ÛŒØ§ÙØª ÛŒÚ© ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ ID"""
    conn = get_db_connection()
    if not conn:
        return None
    
    try:
        cursor = conn.cursor(dictionary=True)
        
        # Ø¯Ø±ÛŒØ§ÙØª ØªØ­Ù„ÛŒÙ„ Ø§ØµÙ„ÛŒ
        cursor.execute("SELECT * FROM analyses WHERE id = %s", (analysis_id,))
        analysis = cursor.fetchone()
        
        if not analysis:
            return None
        
        # ØªØ¨Ø¯ÛŒÙ„ datetime Ø¨Ù‡ string
        analysis['created_at'] = analysis['created_at'].isoformat() if analysis['created_at'] else None
        analysis['analyzed_at'] = analysis['analyzed_at'].isoformat() if analysis['analyzed_at'] else None
        
        # Parse JSON
        if analysis['full_analysis']:
            analysis['full_analysis'] = json.loads(analysis['full_analysis'])
        
        return analysis
        
    except Error as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØªØ­Ù„ÛŒÙ„: {e}")
        return None
    finally:
        cursor.close()
        conn.close()
  
  
def extract_text_from_file(file):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù - Ø¨Ø¯ÙˆÙ† striprtf"""
    filename = file.filename.lower()
    
    try:
        print(f"\nğŸ“ ÙØ§ÛŒÙ„: {filename}")
        
        if filename.endswith('.rtf'):
            # Ø®ÙˆØ§Ù†Ø¯Ù† Ù…Ø­ØªÙˆØ§ÛŒ Ø®Ø§Ù…
            raw_content = file.read()
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¨Ø¯ÙˆÙ† striprtf - Ø±ÙˆØ´ Ù…Ø³ØªÙ‚ÛŒÙ…
            try:
                # ØªÙ„Ø§Ø´ Ø¨Ø§ UTF-8
                content = raw_content.decode('utf-8', errors='ignore')
            except:
                try:
                    # ØªÙ„Ø§Ø´ Ø¨Ø§ Windows-1256 (ÙØ§Ø±Ø³ÛŒ)
                    content = raw_content.decode('windows-1256', errors='ignore')
                except:
                    # Ø¢Ø®Ø±ÛŒÙ† ØªÙ„Ø§Ø´ Ø¨Ø§ latin-1
                    content = raw_content.decode('latin-1', errors='ignore')
            
            # Ø­Ø°Ù Ø¯Ø³ØªÙˆØ±Ø§Øª RTF Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø®Ø§Ù…
            import re
            
            # Ø­Ø°Ù header RTF
            content = re.sub(r'\\rtf\d', '', content)
            content = re.sub(r'\\ansi\\ansicpg\d+', '', content)
            content = re.sub(r'\\deff\d+', '', content)
            
            # Ø­Ø°Ù font table
            content = re.sub(r'\{\\fonttbl[^\}]*\}', '', content)
            
            # Ø­Ø°Ù color table
            content = re.sub(r'\{\\colortbl[^\}]*\}', '', content)
            
            # Ø­Ø°Ù stylesheet
            content = re.sub(r'\{\\stylesheet[^\}]*\}', '', content)
            
            # Ø­Ø°Ù ØªÙ…Ø§Ù… Ø¯Ø³ØªÙˆØ±Ø§Øª RTF (\xxx)
            content = re.sub(r'\\[a-z]+\d*[\s\-]?', '', content)
            
            # Ø­Ø°Ù Ù¾Ø±Ø§Ù†ØªØ²Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
            content = content.replace('{', '').replace('}', '')
            
            # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ
            content = re.sub(r'[\\*]', '', content)
            
            # Ø­Ø°Ù Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ Ù…ØªØ¹Ø¯Ø¯
            content = re.sub(r'\n\s*\n', '\n', content)
            
            text = content.strip()
            
            print(f"âœ… RTF Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯: {len(text)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
            return text
        
        elif filename.endswith('.txt'):
            raw_content = file.read()
            
            # ØªÙ„Ø§Ø´ Ø¨Ø§ encodingâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
            encodings = ['utf-8', 'cp1256', 'windows-1256', 'iso-8859-1', 'latin-1']
            
            for encoding in encodings:
                try:
                    text = raw_content.decode(encoding)
                    print(f"âœ… TXT Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯ ({encoding}): {len(text)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
                    return text
                except:
                    continue
            
            text = raw_content.decode('utf-8', errors='ignore')
            print(f"âœ… TXT Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯ (fallback): {len(text)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
            return text
        
        elif filename.endswith('.pdf'):
            text = ""
            with pdfplumber.open(file) as pdf:
                for page in pdf.pages:
                    text += page.extract_text() or ""
            print(f"âœ… PDF Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯: {len(text)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
            return text
        
        elif filename.endswith('.docx'):
            doc = Document(file)
            text = "\n".join([para.text for para in doc.paragraphs])
            print(f"âœ… DOCX Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯: {len(text)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
            return text
        
        elif filename.endswith(('.xlsx', '.xls')):
            wb = openpyxl.load_workbook(file)
            text = ""
            for sheet in wb.worksheets:
                for row in sheet.iter_rows(values_only=True):
                    text += " ".join([str(cell) for cell in row if cell]) + "\n"
            print(f"âœ… XLSX Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯: {len(text)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
            return text
        
        else:
            # ÙØ§ÛŒÙ„ Ø¹Ù…ÙˆÙ…ÛŒ
            raw_content = file.read()
            encodings = ['utf-8', 'cp1256', 'windows-1256', 'iso-8859-1', 'latin-1']
            
            for encoding in encodings:
                try:
                    text = raw_content.decode(encoding)
                    print(f"âœ… ÙØ§ÛŒÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯ ({encoding}): {len(text)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
                    return text
                except:
                    continue
            
            text = raw_content.decode('utf-8', errors='ignore')
            print(f"âœ… ÙØ§ÛŒÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯ (fallback): {len(text)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
            return text
            
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„: {str(e)}")
        raise Exception(f"Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„: {str(e)}")





def analyze_with_ai(content):
    """ØªØ­Ù„ÛŒÙ„ CRM - Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ú©Ø§Ø±ÛŒ"""
    
    print(f"\n{'='*50}")
    print(f"ğŸ“Š Ø·ÙˆÙ„ Ù…Ø­ØªÙˆØ§: {len(content)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
    print(f"{'='*50}\n")
    
    prompt = f"""Ø§ÛŒÙ† Ú¯Ø²Ø§Ø±Ø´ CRM Ø§Ø³Øª. ØªØ­Ù„ÛŒÙ„ Ú©Ù† Ùˆ **ÙÙ‚Ø· JSON Ø¨Ø±Ú¯Ø±Ø¯ÙˆÙ†** (Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­).

**Ø³ØªÙˆÙ†â€ŒÙ‡Ø§:**
Ø±Ø¯ÛŒÙ | Ø§Ø´ØªØ±Ø§Ú© | Ù†Ø§Ù… | Ù†Ø§Ù… Ù…ÙˆØ³Ø³Ù‡ | ØªÙ„ÙÙ† | Ú©Ø§Ø±Ø¨Ø± | Ø«Ø¨Øª | Ù†ÙˆØ¹ | ÙˆØ¶Ø¹ÛŒØª

**Ù…ØªÙ†:**
{content}

**Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡:**
- Ø¨Ú¯Ùˆ Ú†Ù†Ø¯ ØªÙ…Ø§Ø³ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡ (Ù…ÙˆÙÙ‚ØŒ Ø¨ÛŒâ€ŒÙ¾Ø§Ø³Ø®)
- Ú†Ù‡ Ú©Ø§Ø±Ø´Ù†Ø§Ø³Ø§Ù†ÛŒ ÙØ¹Ø§Ù„ Ø¨ÙˆØ¯Ù†
- Ø¨Ø±ØªØ±ÛŒÙ† Ù…Ø´ØªØ±ÛŒØ§Ù† Ú©Ø¯ÙˆÙ…Ù†
- Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§ØµÙ„ÛŒ Ú†ÛŒ Ø¨ÙˆØ¯Ù†
- Ù†Ù‚Ø§Ø· Ù‚ÙˆØª Ùˆ Ø¶Ø¹Ù

**Ù…Ø«Ø§Ù„ Ø®Ù„Ø§ØµÙ‡:**
"Ú¯Ø²Ø§Ø±Ø´ Ø´Ø§Ù…Ù„ 150 ØªÙ…Ø§Ø³: 90 Ù…ÙˆÙÙ‚ (60%) Ùˆ 30 Ø¨ÛŒâ€ŒÙ¾Ø§Ø³Ø®. Ú©Ø§Ø±Ø´Ù†Ø§Ø³ 'Ù¾Ø§ÛŒØ§Ù†' Ø¨Ø§ 40 ØªÙ…Ø§Ø³ Ø¨Ø±ØªØ±ÛŒÙ† Ø¨ÙˆØ¯. Ù…Ø´ØªØ±ÛŒØ§Ù† Ú©Ù„ÛŒØ¯ÛŒ: Ø§Ø¯Ø§Ø±Ù‡ Ú©Ù„ Ø¯Ø§Ø¯Ú¯Ø³ØªØ±ÛŒ Ùˆ ØªØ§Ø¨Ù„ÙˆÙØ±Ù…Ø§Ù† Ù¾Ø§Ø±. Ù…Ø­ØµÙˆÙ„Ø§Øª: APCØŒ UPSØŒ Ø¯ÙˆØ±Ø¨ÛŒÙ†. Ù†Ù‚Ø§Ø· Ù‚ÙˆØª: Ù¾ÛŒÚ¯ÛŒØ±ÛŒ Ù…Ù†Ø¸Ù… Ùˆ Ø®Ø¯Ù…Ø§Øª ØªØ¹Ù…ÛŒØ±Ø§ØªÛŒ. Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù: ØªÙ…Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒâ€ŒÙ¾Ø§Ø³Ø®."

{{
  "ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ_Ø¹Ø¯Ø¯ÛŒ": {{
    "Ø§Ù…ØªÛŒØ§Ø²_Ú©Ù„": 7,
    "Ø§Ù…ØªÛŒØ§Ø²_Ø¨Ø±Ù‚Ø±Ø§Ø±ÛŒ_Ø§Ø±ØªØ¨Ø§Ø·": 7,
    "Ø§Ù…ØªÛŒØ§Ø²_Ù†ÛŒØ§Ø²Ø³Ù†Ø¬ÛŒ": 6,
    "Ø§Ù…ØªÛŒØ§Ø²_Ø§Ø±Ø²Ø´_ÙØ±ÙˆØ´ÛŒ": 5,
    "Ø§Ù…ØªÛŒØ§Ø²_Ù…Ø¯ÛŒØ±ÛŒØª_Ø§Ø¹ØªØ±Ø§Ø¶": 5,
    "Ø§Ù…ØªÛŒØ§Ø²_Ø´ÙØ§ÙÛŒØª_Ù‚ÛŒÙ…Øª": 6,
    "Ø§Ù…ØªÛŒØ§Ø²_Ø¨Ø³ØªÙ†_ÙØ±ÙˆØ´": 5,
    "Ø§Ù…ØªÛŒØ§Ø²_Ù¾ÛŒÚ¯ÛŒØ±ÛŒ": 8,
    "Ø§Ù…ØªÛŒØ§Ø²_Ù‡Ù…Ø³ÙˆÛŒÛŒ_Ø§Ø­Ø³Ø§Ø³ÛŒ": 6,
    "Ø§Ù…ØªÛŒØ§Ø²_Ø´Ù†ÙˆÙ†Ø¯Ú¯ÛŒ": 7,
    "Ú©ÛŒÙÛŒØª_Ù„ÛŒØ¯_Ø¯Ø±ØµØ¯": 70,
    "ØªØ¹Ø¯Ø§Ø¯_Ø³ÙˆØ§Ù„Ø§Øª_Ø¨Ø§Ø²": 0,
    "ØªØ¹Ø¯Ø§Ø¯_Ø§Ø¹ØªØ±Ø§Ø¶": 5,
    "Ø¯Ø±ØµØ¯_Ù¾Ø§Ø³Ø®_Ù…ÙˆÙÙ‚_Ø¨Ù‡_Ø§Ø¹ØªØ±Ø§Ø¶": 60,
    "ØªØ¹Ø¯Ø§Ø¯_ØªÙ„Ø§Ø´_Ø¨Ø±Ø§ÛŒ_Ø¨Ø³ØªÙ†": 10,
    "Ø§Ù…ØªÛŒØ§Ø²_Ø§Ø­Ø³Ø§Ø³_Ù…Ø´ØªØ±ÛŒ": 6,
    "Ø¢Ù…Ø§Ø¯Ú¯ÛŒ_Ø¨Ø³ØªÙ†_Ø¯Ø±ØµØ¯": 50,
    "Ú†Ú¯Ø§Ù„ÛŒ_Ø§Ø·Ù„Ø§Ø¹Ø§Øª_ÙÙ†ÛŒ_ÙØ±ÙˆØ´Ù†Ø¯Ù‡_Ø¯Ø±ØµØ¯": 75,
    "Ú†Ú¯Ø§Ù„ÛŒ_Ø§Ø·Ù„Ø§Ø¹Ø§Øª_ÙÙ†ÛŒ_Ù…Ø´ØªØ±ÛŒ_Ø¯Ø±ØµØ¯": 60,
    "disc_d": 6,
    "disc_i": 7,
    "disc_s": 6,
    "disc_c": 5,
    "Ø­Ø³Ø§Ø³ÛŒØª_Ù‚ÛŒÙ…Øª_Ù…Ø´ØªØ±ÛŒ_Ø¯Ø±ØµØ¯": 65,
    "Ø­Ø³Ø§Ø³ÛŒØª_Ø±ÛŒØ³Ú©_Ù…Ø´ØªØ±ÛŒ_Ø¯Ø±ØµØ¯": 55,
    "Ø­Ø³Ø§Ø³ÛŒØª_Ø²Ù…Ø§Ù†_Ù…Ø´ØªØ±ÛŒ_Ø¯Ø±ØµØ¯": 60,
    "ØªØ¹Ø¯Ø§Ø¯_Ø¨Ù„Ù‡_Ù¾Ù„Ù‡_Ø§ÛŒ": 3
  }},
  "ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ_Ù…ØªÙ†ÛŒ": {{
    "Ù†Ø§Ù…_ÙØ±ÙˆØ´Ù†Ø¯Ù‡": "Ù¾Ø§ÛŒØ§Ù†ØŒ Ú©Ø§Ø±Ú¯Ø±ØŒ Ø­Ø³ÛŒÙ†ÛŒ",
    "Ú©Ø¯_ÙØ±ÙˆØ´Ù†Ø¯Ù‡": "",
    "Ù†Ø§Ù…_Ù…Ø´ØªØ±ÛŒ": "Ø§Ø¯Ø§Ø±Ù‡ Ú©Ù„ Ø¯Ø§Ø¯Ú¯Ø³ØªØ±ÛŒ Ù…Ø´Ù‡Ø¯ØŒ ØªØ§Ø¨Ù„ÙˆÙØ±Ù…Ø§Ù† Ù¾Ø§Ø±",
    "Ù…Ø¯Øª_ØªÙ…Ø§Ø³": "",
    "Ù†ÙˆØ¹_ØªÙ…Ø§Ø³_Ø¬Ù‡Øª": "Ø®Ø±ÙˆØ¬ÛŒ",
    "Ù†ÙˆØ¹_ØªÙ…Ø§Ø³_Ù…Ø±Ø­Ù„Ù‡": "Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ùˆ ÙØ±ÙˆØ´",
    "Ù†ÙˆØ¹_ØªÙ…Ø§Ø³_Ú¯Ø±Ù…ÛŒ": "Ù…ØªÙˆØ³Ø·",
    "Ù†ÙˆØ¹_ØªÙ…Ø§Ø³_Ù…Ø§Ù‡ÛŒØª": "Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ùˆ ÙØ±ÙˆØ´",
    "Ù…Ø­ØµÙˆÙ„": "APCØŒ UPSØŒ Ø¯ÙˆØ±Ø¨ÛŒÙ†ØŒ Ø³Ø§Ù†ØªØ±Ø§Ù„",
    "Ø³Ø·Ø­_ÙØ±ÙˆØ´Ù†Ø¯Ù‡": "Ù…ØªÙˆØ³Ø·",
    "disc_ØªÛŒÙ¾": "I",
    "disc_Ø´ÙˆØ§Ù‡Ø¯": ["ØªØ¹Ø§Ù…Ù„ Ø²ÛŒØ§Ø¯", "Ù¾ÛŒÚ¯ÛŒØ±ÛŒ Ù…Ø³ØªÙ…Ø±"],
    "disc_Ø±Ø§Ù‡Ù†Ù…Ø§": "ØªØ¹Ø§Ù…Ù„ Ù…Ø³ØªÙ…Ø± Ùˆ Ù¾ÛŒÚ¯ÛŒØ±ÛŒ",
    "ØªØ±Ø¬ÛŒØ­_Ú©Ø§Ù†Ø§Ù„": "ØªÙ„ÙÙ†",
    "Ø³Ø·Ø­_Ø¢Ú¯Ø§Ù‡ÛŒ_Ù…Ø´ØªØ±ÛŒ": "Ù…ØªÙˆØ³Ø·",
    "Ù†Ø³Ø¨Øª_Ø²Ù…Ø§Ù†_ØµØ­Ø¨Øª_Ù…Ø´ØªØ±ÛŒ_Ø¨Ù‡_ÙØ±ÙˆØ´Ù†Ø¯Ù‡": "40:60",
    "Ù†Ø³Ø¨Øª_Ø²Ù…Ø§Ù†_ØµØ­Ø¨Øª_ÙØ±ÙˆØ´Ù†Ø¯Ù‡_Ø¨Ù‡_Ù…Ø´ØªØ±ÛŒ": "60:40",
    "Ø®Ù„Ø§ØµÙ‡": "Ø®Ù„Ø§ØµÙ‡ Ú©Ø§Ù…Ù„ Ù…Ø·Ø§Ø¨Ù‚ Ù…Ø«Ø§Ù„ - Ø¨Ø§ Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Ø¬Ø²Ø¦ÛŒØ§Øª",
    "ØªØ­Ù„ÛŒÙ„_Ø´Ø®ØµÛŒØª_Ù…Ø´ØªØ±ÛŒ": "Ù…Ø´ØªØ±ÛŒØ§Ù† Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ Ùˆ Ø¯ÙˆÙ„ØªÛŒ Ø¨Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…Ø³ØªÙ…Ø±",
    "Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ_Ø¹Ù…Ù„Ú©Ø±Ø¯_ÙØ±Ø¯ÛŒ_ÙØ±ÙˆØ´Ù†Ø¯Ù‡": "ØªÛŒÙ… ÙØ¹Ø§Ù„ Ø¨Ø§ Ù¾ÛŒÚ¯ÛŒØ±ÛŒ Ù…Ù†Ø¸Ù…",
    "ØªØ´Ø®ÛŒØµ_Ø¢Ù…Ø§Ø¯Ú¯ÛŒ": "Ø¢Ù…Ø§Ø¯Ú¯ÛŒ Ù…ØªÙˆØ³Ø· Ø¨Ø±Ø§ÛŒ Ø®Ø±ÛŒØ¯",
    "Ø§Ù‚Ø¯Ø§Ù…_Ø¨Ø¹Ø¯ÛŒ": "Ù¾ÛŒÚ¯ÛŒØ±ÛŒ ØªÙ…Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒâ€ŒÙ¾Ø§Ø³Ø® Ùˆ Ø¨Ø³ØªÙ† ÙØ±ÙˆØ´â€ŒÙ‡Ø§"
  }},
  "Ø¯Ù„Ø§ÛŒÙ„_Ú©Ø§Ù‡Ø´_Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§": {{
    "Ø¨Ø±Ù‚Ø±Ø§Ø±ÛŒ_Ø§Ø±ØªØ¨Ø§Ø·": ["ØªÙ…Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒâ€ŒÙ¾Ø§Ø³Ø®"],
    "Ù†ÛŒØ§Ø²Ø³Ù†Ø¬ÛŒ": ["Ø¹Ø¯Ù… Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ø§Ù…Ù„ Ù†ÛŒØ§Ø²"],
    "Ø§Ø±Ø²Ø´_ÙØ±ÙˆØ´ÛŒ": ["Ø¹Ø¯Ù… ØªÙˆØ¶ÛŒØ­ Ú©Ø§Ù…Ù„ Ø§Ø±Ø²Ø´"],
    "Ù…Ø¯ÛŒØ±ÛŒØª_Ø§Ø¹ØªØ±Ø§Ø¶": ["Ø¨Ø±Ø®ÛŒ Ø§Ø¹ØªØ±Ø§Ø¶Ø§Øª Ø¨Ø¯ÙˆÙ† Ù¾Ø§Ø³Ø®"],
    "Ø´ÙØ§ÙÛŒØª_Ù‚ÛŒÙ…Øª": ["ØªØ§Ø®ÛŒØ± Ø¯Ø± Ø§Ø±Ø³Ø§Ù„ Ù‚ÛŒÙ…Øª"],
    "Ø¨Ø³ØªÙ†_ÙØ±ÙˆØ´": ["Ø¹Ø¯Ù… Ø¨Ø³ØªÙ† ÙØ±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡"],
    "Ù¾ÛŒÚ¯ÛŒØ±ÛŒ": ["Ø®ØªÙ… Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù…"],
    "Ù‡Ù…Ø³ÙˆÛŒÛŒ_Ø§Ø­Ø³Ø§Ø³ÛŒ": [],
    "Ø´Ù†ÙˆÙ†Ø¯Ú¯ÛŒ": []
  }},
  "Ø¯Ù„Ø§ÛŒÙ„_Ú©Ø³Ø¨_Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§": {{
    "Ø¨Ø±Ù‚Ø±Ø§Ø±ÛŒ_Ø§Ø±ØªØ¨Ø§Ø·": ["ØªÙ…Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø¸Ù…"],
    "Ù†ÛŒØ§Ø²Ø³Ù†Ø¬ÛŒ": ["Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†ÛŒØ§Ø²Ù‡Ø§ÛŒ ÙÙ†ÛŒ"],
    "Ø§Ø±Ø²Ø´_ÙØ±ÙˆØ´ÛŒ": ["Ø§Ø±Ø§Ø¦Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª Ù…ØªÙ†ÙˆØ¹"],
    "Ù…Ø¯ÛŒØ±ÛŒØª_Ø§Ø¹ØªØ±Ø§Ø¶": ["Ø±Ø³ÛŒØ¯Ú¯ÛŒ Ø¨Ù‡ Ù…Ø´Ú©Ù„Ø§Øª"],
    "Ø´ÙØ§ÙÛŒØª_Ù‚ÛŒÙ…Øª": ["Ø§Ø±Ø§Ø¦Ù‡ Ù‚ÛŒÙ…Øª"],
    "Ø¨Ø³ØªÙ†_ÙØ±ÙˆØ´": ["ÙØ§Ú©ØªÙˆØ±Ù‡Ø§ÛŒ Ù…ÙˆÙÙ‚"],
    "Ù¾ÛŒÚ¯ÛŒØ±ÛŒ": ["Reminder Ù…Ù†Ø¸Ù…"],
    "Ù‡Ù…Ø³ÙˆÛŒÛŒ_Ø§Ø­Ø³Ø§Ø³ÛŒ": ["Ø±ÙØªØ§Ø± Ù…Ø­ØªØ±Ù…Ø§Ù†Ù‡"],
    "Ø´Ù†ÙˆÙ†Ø¯Ú¯ÛŒ": ["ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù†ÛŒØ§Ø²Ù‡Ø§"]
  }},
  "Ù„ÛŒØ³Øª_Ù‡Ø§": {{
    "Ú©Ù„Ù…Ø§Øª_Ù…Ø«Ø¨Øª": ["ØªØ§ÛŒÛŒØ¯", "Ù…ÙˆÙÙ‚", "Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯", "Ù‚Ø¨ÙˆÙ„"],
    "Ú©Ù„Ù…Ø§Øª_Ù…Ù†ÙÛŒ": ["Ø¨ÛŒâ€ŒÙ¾Ø§Ø³Ø®", "Ø®Ø§ØªÙ…Ù‡", "Ù…Ø´Ú©Ù„", "ØªØ§Ø®ÛŒØ±"],
    "Ø±ÛŒØ³Ú©_Ù‡Ø§": ["Ø§Ø² Ø¯Ø³Øª Ø¯Ø§Ø¯Ù† Ù…Ø´ØªØ±ÛŒ", "ØªØ§Ø®ÛŒØ± Ø¯Ø± Ù¾Ø§Ø³Ø®"],
    "Ù†Ù‚Ø§Ø·_Ù‚ÙˆØª": ["Ù¾ÛŒÚ¯ÛŒØ±ÛŒ Ù…Ù†Ø¸Ù…", "ØªÙ†ÙˆØ¹ Ø®Ø¯Ù…Ø§Øª", "ØªØ¹Ù…ÛŒØ±Ø§Øª ÙØ¹Ø§Ù„"],
    "Ù†Ù‚Ø§Ø·_Ø¶Ø¹Ù": ["ØªÙ…Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒâ€ŒÙ¾Ø§Ø³Ø®", "Ø®ØªÙ… Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù…"],
    "Ø§Ø¹ØªØ±Ø§Ø¶Ø§Øª": ["ØªØ§Ø®ÛŒØ± Ø¯Ø± Ù¾Ø§Ø³Ø®", "Ù…Ø´Ú©Ù„ Ø¯Ø± ØªØ­ÙˆÛŒÙ„"],
    "ØªÚ©Ù†ÛŒÚ©Ù‡Ø§": ["Reminder", "Ø§Ø±Ø¬Ø§Ø¹ Ø¨Ù‡ Ø­Ø³Ø§Ø¨Ø¯Ø§Ø±ÛŒ", "Ù¾ÛŒÚ¯ÛŒØ±ÛŒ ØªÙ„ÙÙ†ÛŒ"],
    "Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ_Ø±Ø¹Ø§ÛŒØª_Ù†Ø´Ø¯Ù‡": ["Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø®"],
    "Ø§Ø´ØªØ¨Ø§Ù‡Ø§Øª_Ø±Ø§ÛŒØ¬": ["Ø¹Ø¯Ù… Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ù…ÙˆÙ‚Ø¹"]
  }},
  "Ø¢Ù…Ø§Ø±": {{
    "ØªØ¹Ø¯Ø§Ø¯_Ú©Ù„_ØªÙ…Ø§Ø³_Ù‡Ø§": 150,
    "ØªÙ…Ø§Ø³_Ù‡Ø§ÛŒ_Ù…ÙˆÙÙ‚": 90,
    "ØªÙ…Ø§Ø³_Ù‡Ø§ÛŒ_Ø¨ÛŒ_Ù¾Ø§Ø³Ø®": 30,
    "ØªÙ…Ø§Ø³_Ù‡Ø§ÛŒ_Ø§Ø±Ø¬Ø§Ø¹ÛŒ": 20,
    "Ú©Ø§Ø±Ø¨Ø±Ø§Ù†_ÙØ¹Ø§Ù„": [
      {{"Ù†Ø§Ù…": "Ù¾Ø§ÛŒØ§Ù†", "ØªØ¹Ø¯Ø§Ø¯_ØªÙ…Ø§Ø³": 40, "ÛŒØ§Ø¯Ø¯Ø§Ø´Øª_Ø¹Ù…Ù„Ú©Ø±Ø¯": "Ø¨Ø±ØªØ±ÛŒÙ† Ú©Ø§Ø±Ø´Ù†Ø§Ø³"}},
      {{"Ù†Ø§Ù…": "ÙÙ†ÛŒ-Ø§Ø¯Ø§Ø±ÛŒ1", "ØªØ¹Ø¯Ø§Ø¯_ØªÙ…Ø§Ø³": 25, "ÛŒØ§Ø¯Ø¯Ø§Ø´Øª_Ø¹Ù…Ù„Ú©Ø±Ø¯": "Ø®ÙˆØ¨"}},
      {{"Ù†Ø§Ù…": "Ø­Ø³ÛŒÙ†ÛŒ", "ØªØ¹Ø¯Ø§Ø¯_ØªÙ…Ø§Ø³": 20, "ÛŒØ§Ø¯Ø¯Ø§Ø´Øª_Ø¹Ù…Ù„Ú©Ø±Ø¯": "ÙØ¹Ø§Ù„"}},
      {{"Ù†Ø§Ù…": "Ú©Ø§Ø±Ú¯Ø±", "ØªØ¹Ø¯Ø§Ø¯_ØªÙ…Ø§Ø³": 15, "ÛŒØ§Ø¯Ø¯Ø§Ø´Øª_Ø¹Ù…Ù„Ú©Ø±Ø¯": "Ø®ÙˆØ¨"}},
      {{"Ù†Ø§Ù…": "Ø±Ø³ÙˆÙ„ÛŒ", "ØªØ¹Ø¯Ø§Ø¯_ØªÙ…Ø§Ø³": 10, "ÛŒØ§Ø¯Ø¯Ø§Ø´Øª_Ø¹Ù…Ù„Ú©Ø±Ø¯": "Ù…ØªÙˆØ³Ø·"}}
    ],
    "Ù…Ø´ØªØ±ÛŒØ§Ù†_Ù¾Ø±ØªÙ…Ø§Ø³": [
      {{"Ù†Ø§Ù…": "Ø§Ø¯Ø§Ø±Ù‡ Ú©Ù„ Ø¯Ø§Ø¯Ú¯Ø³ØªØ±ÛŒ Ù…Ø´Ù‡Ø¯", "ØªØ¹Ø¯Ø§Ø¯_ØªÙ…Ø§Ø³": 12, "Ú©ÛŒÙÛŒØª_ØªØ¹Ø§Ù…Ù„": "Ø¹Ø§Ù„ÛŒ"}},
      {{"Ù†Ø§Ù…": "ØªØ§Ø¨Ù„ÙˆÙØ±Ù…Ø§Ù† Ù¾Ø§Ø±", "ØªØ¹Ø¯Ø§Ø¯_ØªÙ…Ø§Ø³": 8, "Ú©ÛŒÙÛŒØª_ØªØ¹Ø§Ù…Ù„": "Ø®ÙˆØ¨"}},
      {{"Ù†Ø§Ù…": "Ø´Ø±Ú©Øª Ú¯Ø§Ø²", "ØªØ¹Ø¯Ø§Ø¯_ØªÙ…Ø§Ø³": 6, "Ú©ÛŒÙÛŒØª_ØªØ¹Ø§Ù…Ù„": "Ù…ØªÙˆØ³Ø·"}}
    ],
    "Ø§Ù†ÙˆØ§Ø¹_ØªÙ…Ø§Ø³": {{
      "Ù¾Ø§ÛŒØ§Ù†": 50,
      "Reminder": 40,
      "Erja": 20,
      "ØªØ¹Ù…ÛŒØ±Ø§Øª": 30,
      "Repair": 10
    }}
  }},
  "Ø¨Ù‡ØªØ±ÛŒÙ†_Ù‡Ø§": {{
    "Ø¨Ù‡ØªØ±ÛŒÙ†_ÙØ±ÙˆØ´Ù†Ø¯Ù‡": {{
      "Ù†Ø§Ù…": "Ù¾Ø§ÛŒØ§Ù†",
      "Ø¯Ù„ÛŒÙ„": "40 ØªÙ…Ø§Ø³ Ø¨Ø§ Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ù„Ø§"
    }},
    "Ø¨Ù‡ØªØ±ÛŒÙ†_Ù…Ø´ØªØ±ÛŒ": {{
      "Ù†Ø§Ù…": "Ø§Ø¯Ø§Ø±Ù‡ Ú©Ù„ Ø¯Ø§Ø¯Ú¯Ø³ØªØ±ÛŒ Ù…Ø´Ù‡Ø¯",
      "Ø¯Ù„ÛŒÙ„": "12 ØªÙ…Ø§Ø³ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¹Ø§Ù„ÛŒ"
    }}
  }}
}}"""

    try:
        print(f"ğŸ“¤ Ø§Ø±Ø³Ø§Ù„...")
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system", 
                    "content": "You are a CRM analyst. Return ONLY JSON with this exact structure. Make the summary detailed with specific numbers. No markdown, no explanation."
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            temperature=0.3,
            max_tokens=4000
        )
        
        response_text = response.choices[0].message.content.strip()
        
        print(f"âœ… Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯")
        print(f"\n{'='*50}")
        print(f"ğŸ¤– Ù¾Ø§Ø³Ø®:")
        print(response_text[:500] + "...")
        print(f"{'='*50}\n")
        
        # Ø­Ø°Ù markdown
        if response_text.startswith('```'):
            lines = response_text.split('\n')
            json_lines = []
            in_json = False
            for line in lines:
                if line.strip() == '```json' or line.strip() == '```':
                    in_json = not in_json
                    continue
                if in_json or (line.strip().startswith('{') or json_lines):
                    json_lines.append(line)
            response_text = '\n'.join(json_lines).strip()
        
        # Parse
        analysis = json.loads(response_text)
        
        print(f"âœ… JSON Ù¾Ø§Ø±Ø³ Ø´Ø¯")
        print(f"  Ø§Ù…ØªÛŒØ§Ø²: {analysis['ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ_Ø¹Ø¯Ø¯ÛŒ']['Ø§Ù…ØªÛŒØ§Ø²_Ú©Ù„']}")
        print(f"  ØªÙ…Ø§Ø³â€ŒÙ‡Ø§: {analysis['Ø¢Ù…Ø§Ø±']['ØªØ¹Ø¯Ø§Ø¯_Ú©Ù„_ØªÙ…Ø§Ø³_Ù‡Ø§']}")
        print(f"  Ø®Ù„Ø§ØµÙ‡: {analysis['ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ_Ù…ØªÙ†ÛŒ']['Ø®Ù„Ø§ØµÙ‡'][:80]}...")
        print()
        
        return analysis
        
    except json.JSONDecodeError as e:
        print(f"âŒ JSON Ø®Ø·Ø§: {str(e)}")
        print(f"ğŸ“„ Ù…ØªÙ†:")
        print(response_text[:1000])
        return {"error": True, "message": "Ø®Ø·Ø§ Ø¯Ø± JSON"}
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"error": True, "message": str(e)}



@app.route('/')
def index():
    return render_template('index.html')

@app.route('/history')
def history():
    return render_template('history.html')


@app.route('/users')
def users_page():
    """ØµÙØ­Ù‡ ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ø±Ø´Ù†Ø§Ø³Ø§Ù†"""
    return render_template('users.html')


@app.route('/referral-history')
def referral_history_page():
    """ØµÙØ­Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø§Ø±Ø¬Ø§Ø¹ÛŒØ§Øª"""
    return render_template('referral_history.html')


@app.route('/api/referral-history')
def get_referral_history():
    """Ø¯Ø±ÛŒØ§ÙØª ØªØ§Ø±ÛŒØ®Ú†Ù‡ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø¬Ø§Ø¹ÛŒØ§Øª"""
    try:
        conn = get_db_connection()
        if not conn:
            return jsonify([])
        
        cursor = conn.cursor(dictionary=True)
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¬Ø¯ÙˆÙ„
        cursor.execute("SHOW TABLES LIKE 'referral_analyses'")
        if not cursor.fetchone():
            print("âš ï¸ Ø¬Ø¯ÙˆÙ„ referral_analyses ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
            return jsonify([])
        
        query = """
        SELECT 
            id,
            file_name,
            DATE_FORMAT(analyzed_at, '%%Y-%%m-%%d %%H:%%i:%%s') as analyzed_at,
            total_referrals,
            completed_count,
            pending_count,
            ROUND(completion_rate, 1) as completion_rate,
            JSON_UNQUOTE(JSON_EXTRACT(full_analysis, '$.status_analysis.worst_sender_pending.unit')) as bottleneck_unit
        FROM referral_analyses
        ORDER BY analyzed_at DESC
        """
        
        cursor.execute(query)
        results = cursor.fetchall()
        
        print(f"âœ… {len(results)} Ø±Ú©ÙˆØ±Ø¯ Ø§Ø² ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø§Ø±Ø¬Ø§Ø¹ÛŒØ§Øª Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯")
        
        cursor.close()
        conn.close()
        
        return jsonify(results)
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± get_referral_history: {str(e)}")
        return jsonify([])


@app.route('/api/referral-analysis/<int:analysis_id>')
def get_referral_analysis(analysis_id):
    """Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø²Ø¦ÛŒØ§Øª ÛŒÚ© ØªØ­Ù„ÛŒÙ„"""
    try:
        conn = get_db_connection()
        if not conn:
            return jsonify({"error": "Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„"}), 500
        
        cursor = conn.cursor(dictionary=True)
        
        query = """
        SELECT 
            id,
            file_name,
            analyzed_at,
            total_referrals,
            completed_count,
            pending_count,
            completion_rate,
            full_analysis
        FROM referral_analyses
        WHERE id = %s
        """
        
        cursor.execute(query, (analysis_id,))
        result = cursor.fetchone()
        cursor.close()
        conn.close()
        
        if not result:
            return jsonify({"error": "ÛŒØ§ÙØª Ù†Ø´Ø¯"}), 404
        
        # ØªØ¨Ø¯ÛŒÙ„ JSON
        if result['full_analysis']:
            result['full_analysis'] = json.loads(result['full_analysis'])
        
        return jsonify(result)
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§: {str(e)}")
        return jsonify({"error": str(e)}), 500




@app.route('/api/referral-report/<int:analysis_id>')
def download_referral_report(analysis_id):
    """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú¯Ø²Ø§Ø±Ø´ Excel Ø§Ø² ØªØ­Ù„ÛŒÙ„ Ø§Ø±Ø¬Ø§Ø¹ÛŒØ§Øª"""
    try:
        conn = get_db_connection()
        if not conn:
            return jsonify({"error": "Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„"}), 500
        
        cursor = conn.cursor(dictionary=True)
        cursor.execute("SELECT * FROM referral_analyses WHERE id = %s", (analysis_id,))
        analysis = cursor.fetchone()
        cursor.close()
        conn.close()
        
        if not analysis:
            return jsonify({"error": "ÛŒØ§ÙØª Ù†Ø´Ø¯"}), 404
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Excel
        import pandas as pd
        from io import BytesIO
        
        output = BytesIO()
        
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            # Ø¨Ø±Ú¯Ù‡ Ø®Ù„Ø§ØµÙ‡
            summary_df = pd.DataFrame([{
                'Ù†Ø§Ù… ÙØ§ÛŒÙ„': analysis['file_name'],
                'ØªØ§Ø±ÛŒØ® ØªØ­Ù„ÛŒÙ„': analysis['analyzed_at'],
                'Ú©Ù„ Ø§Ø±Ø¬Ø§Ø¹Ø§Øª': analysis['total_referrals'],
                'Ø§ØªÙ…Ø§Ù… ÛŒØ§ÙØªÙ‡': analysis['completed_count'],
                'Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø´Ø¯Ù‡': analysis['pending_count'],
                'Ø¯Ø±ØµØ¯ Ù…ÙˆÙÙ‚ÛŒØª': f"{analysis['completion_rate']:.1f}%"
            }])
            summary_df.to_excel(writer, sheet_name='Ø®Ù„Ø§ØµÙ‡', index=False)
            
            # Ø¨Ø±Ú¯Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª (Ø§Ø² full_analysis)
            if analysis['full_analysis']:
                full = json.loads(analysis['full_analysis'])
                
                # ÙˆØ¶Ø¹ÛŒØªâ€ŒÙ‡Ø§
                status_dist = full.get('status_analysis', {}).get('status_distribution', {})
                if status_dist:
                    status_df = pd.DataFrame([
                        {'ÙˆØ¶Ø¹ÛŒØª': k, 'ØªØ¹Ø¯Ø§Ø¯': v} 
                        for k, v in status_dist.items()
                    ])
                    status_df.to_excel(writer, sheet_name='ÙˆØ¶Ø¹ÛŒØªâ€ŒÙ‡Ø§', index=False)
                
                # Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
                subjects = full.get('subject_analysis', {}).get('unique_subjects', [])
                if subjects:
                    subject_df = pd.DataFrame(subjects)
                    subject_df.to_excel(writer, sheet_name='Ù…ÙˆØ¶ÙˆØ¹Ø§Øª', index=False)
                
                # ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§
                recs = full.get('comprehensive_insights', {}).get('recommendations_fa', [])
                if recs:
                    rec_df = pd.DataFrame({'ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§': recs})
                    rec_df.to_excel(writer, sheet_name='ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§', index=False)
        
        output.seek(0)
        
        return send_file(
            output,
            as_attachment=True,
            download_name=f"referral_report_{analysis_id}.xlsx",
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± download_referral_report: {str(e)}")
        return jsonify({"error": str(e)}), 500




def save_referral_analysis(file_info, analysis_data):
    """Ø°Ø®ÛŒØ±Ù‡ ØªØ­Ù„ÛŒÙ„ Ø§Ø±Ø¬Ø§Ø¹ÛŒØ§Øª Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø§ Ø¯ÛŒØ¨Ø§Ú¯ Ú©Ø§Ù…Ù„"""
    conn = get_db_connection()
    if not conn:
        print("âŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±Ù‚Ø±Ø§Ø± Ù†Ø´Ø¯")
        return None
    
    try:
        cursor = conn.cursor()
        
        # Ø¯ÛŒØ¨Ø§Ú¯ - Ú†Ø§Ù¾ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø±ÛŒØ§ÙØªÛŒ
        print("\n" + "="*60)
        print("ğŸ“Š STRUCTURE RECEIVED FROM OPENAI:")
        print(json.dumps(analysis_data, indent=2, ensure_ascii=False)[:1000])
        print("="*60 + "\n")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ø¯Ø³ØªØ±Ø³ÛŒ Ø§ÛŒÙ…Ù†
        status = analysis_data.get('status_analysis', {})
        if not status:
            print("âš ï¸ status_analysis Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ø§Ø² Ú©Ù„ Ø¯Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù…")
            status = analysis_data
        
        dist = status.get('status_distribution', {})
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø±
        total = 0
        completed = 0
        pending = 0
        in_progress = 0
        seen = 0
        accepted = 0
        
        # Ø§Ú¯Ø± dist Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨ÙˆØ¯
        if isinstance(dist, dict):
            total = sum(dist.values())
            completed = dist.get('Ø§ØªÙ…Ø§Ù… Ú©Ø§Ø±', 0)
            pending = dist.get('Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø´Ø¯Ù‡', 0)
            in_progress = dist.get('Ø¯Ø±Ø­Ø§Ù„ Ù¾ÛŒÚ¯ÛŒØ±ÛŒ', 0)
            seen = dist.get('Ø±ÙˆÛŒØª Ø´Ø¯Ù‡', 0)
            accepted = dist.get('Ù‚Ø¨ÙˆÙ„ Ø§Ø±Ø¬Ø§Ø¹', 0)
        else:
            print(f"âš ï¸ dist Ø§Ø² Ù†ÙˆØ¹ {type(dist)} Ø§Ø³ØªØŒ Ù†Ù‡ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ")
            # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² Ø¬Ø§ÛŒ Ø¯ÛŒÚ¯Ø±
            if isinstance(analysis_data, dict):
                for key in ['status_distribution', 'distribution', 'statuses']:
                    if key in analysis_data and isinstance(analysis_data[key], dict):
                        dist = analysis_data[key]
                        total = sum(dist.values())
                        completed = dist.get('Ø§ØªÙ…Ø§Ù… Ú©Ø§Ø±', 0)
                        pending = dist.get('Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø´Ø¯Ù‡', 0)
                        break
        
        completion_rate = (completed / total * 100) if total > 0 else 0
        pending_rate = (pending / total * 100) if total > 0 else 0
        
        print(f"ğŸ“ˆ Ø¢Ù…Ø§Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡:")
        print(f"   total: {total}")
        print(f"   completed: {completed}")
        print(f"   pending: {pending}")
        print(f"   in_progress: {in_progress}")
        print(f"   seen: {seen}")
        print(f"   accepted: {accepted}")
        print(f"   completion_rate: {completion_rate:.1f}%")
        print(f"   pending_rate: {pending_rate:.1f}%")
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¬Ø¯ÙˆÙ„
        cursor.execute("SHOW TABLES LIKE 'referral_analyses'")
        table_exists = cursor.fetchone()
        
        if not table_exists:
            print("âŒ Ø¬Ø¯ÙˆÙ„ referral_analyses ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯!")
            print("ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø§ÛŒØ¬Ø§Ø¯ Ø¬Ø¯ÙˆÙ„...")
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ø¬Ø¯ÙˆÙ„
            cursor.execute("""
                CREATE TABLE referral_analyses (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    file_name VARCHAR(255),
                    file_path VARCHAR(500),
                    file_size INT,
                    analyzed_at DATETIME,
                    total_referrals INT,
                    completed_count INT,
                    pending_count INT,
                    in_progress_count INT,
                    seen_count INT,
                    accepted_count INT,
                    completion_rate FLOAT,
                    pending_rate FLOAT,
                    full_analysis JSON,
                    created_at DATETIME DEFAULT NOW()
                )
            """)
            print("âœ… Ø¬Ø¯ÙˆÙ„ referral_analyses Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")
        
        # Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡
        query = """
        INSERT INTO referral_analyses (
            file_name, file_path, file_size, analyzed_at,
            total_referrals, completed_count, pending_count,
            in_progress_count, seen_count, accepted_count,
            completion_rate, pending_rate, full_analysis
        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        
        values = (
            file_info['name'], file_info['path'], file_info['size'], datetime.now(),
            total, completed, pending, in_progress, seen, accepted,
            completion_rate, pending_rate,
            json.dumps(analysis_data, ensure_ascii=False)
        )
        
        print("ğŸ“¤ Ø§Ø¬Ø±Ø§ÛŒ query...")
        cursor.execute(query, values)
        analysis_id = cursor.lastrowid
        conn.commit()
        
        print(f"âœ… ØªØ­Ù„ÛŒÙ„ Ø§Ø±Ø¬Ø§Ø¹ÛŒØ§Øª Ø¨Ø§ ID {analysis_id} Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¯Ø± Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ø¨Ø³ØªÙ‡ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        try:
            # Ø°Ø®ÛŒØ±Ù‡ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
            subjects = analysis_data.get('subject_analysis', {}).get('unique_subjects', [])
            if subjects and analysis_id:
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS referral_subjects (
                        id INT PRIMARY KEY AUTO_INCREMENT,
                        analysis_id INT,
                        subject_name VARCHAR(255),
                        frequency INT,
                        FOREIGN KEY (analysis_id) REFERENCES referral_analyses(id) ON DELETE CASCADE
                    )
                """)
                for subj in subjects:
                    cursor.execute(
                        "INSERT INTO referral_subjects (analysis_id, subject_name, frequency) VALUES (%s, %s, %s)",
                        (analysis_id, subj.get('subject'), subj.get('count'))
                    )
                conn.commit()
                print(f"âœ… {len(subjects)} Ù…ÙˆØ¶ÙˆØ¹ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª: {e}")
        
        return analysis_id
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡: {type(e).__name__}: {str(e)}")
        import traceback
        traceback.print_exc()
        if conn:
            conn.rollback()
        return None
    finally:
        if cursor:
            cursor.close()



# ========================================
# REFERRAL ANALYSIS MODULE
# ========================================
@app.route('/api/analyze-referral', methods=['POST'])
def analyze_referral():
    """API Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙØ§ÛŒÙ„ Ø§Ø±Ø¬Ø§Ø¹ÛŒØ§Øª - Ù†Ø³Ø®Ù‡ Ú©Ø§Ù…Ù„ Ø¨Ø§ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
    file_path = None
    
    try:
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„
        if 'file' not in request.files:
            return jsonify({"error": "ÙØ§ÛŒÙ„ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª"}), 400
        
        file = request.files['file']
        
        if file.filename == '':            return jsonify({"error": "ÙØ§ÛŒÙ„ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª"}), 400
        
        # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_filename = f"referral_{timestamp}_{file.filename}"
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], safe_filename)
        
        file.save(file_path)
        print(f"âœ… ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {file_path}")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ†
        with open(file_path, 'rb') as f:
            from io import BytesIO
            file_obj = BytesIO(f.read())
            file_obj.filename = file.filename
            
            content = extract_text_from_file(file_obj)
        
        if not content or len(content.strip()) < 50:
            os.remove(file_path)
            return jsonify({"error": "Ù…Ø­ØªÙˆØ§ÛŒ ÙØ§ÛŒÙ„ Ø®Ø§Ù„ÛŒ ÛŒØ§ Ù†Ø§Ù‚Øµ Ø§Ø³Øª"}), 400
        
        # ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ AI
        analysis = analyze_referral_with_ai(content)
        
        if analysis.get('error'):
            os.remove(file_path)
            return jsonify(analysis), 400
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡
        file_info = {
            'name': file.filename,
            'path': file_path,
            'size': os.path.getsize(file_path),
            'type': file.filename.rsplit('.', 1)[1].lower() if '.' in file.filename else 'unknown'
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        analysis_id = save_referral_analysis(file_info, analysis)
        
        if analysis_id:
            analysis['id'] = analysis_id
            print(f"âœ… ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ ID {analysis_id} Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        else:
            print("âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³")
        
        return jsonify(analysis)
        
    except Exception as e:
        print(f"\nâŒâŒâŒ Ø®Ø·Ø§ÛŒ CRITICAL Ø¯Ø± analyze_referral: {type(e).__name__}")
        print(f"Ù¾ÛŒØ§Ù…: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # Ø­Ø°Ù ÙØ§ÛŒÙ„ Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§
        if file_path and os.path.exists(file_path):
            os.remove(file_path)
        
        return jsonify({
            "error": True,
            "message": f"{type(e).__name__}: {str(e)}"
        }), 500





def analyze_referral_with_ai(content):
    """ØªØ­Ù„ÛŒÙ„ ÙØ§ÛŒÙ„ Ø§Ø±Ø¬Ø§Ø¹ÛŒØ§Øª Ø¨Ø§ OpenAI - Ù¾Ø±Ø§Ù…Ù¾Øª Ú©Ø§Ù…Ù„ Ø¨Ø§ ØªÙ…Ø§Ù… Ø³ÙˆØ§Ù„Ø§Øª"""
    
    print(f"\n{'='*50}")
    print(f"ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ø§Ø±Ø¬Ø§Ø¹ÛŒØ§Øª - Ø·ÙˆÙ„ Ù…Ø­ØªÙˆØ§: {len(content)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
    print(f"{'='*50}\n")
    
    # Ù¾Ø±Ø§Ù…Ù¾Øª Ú©Ø§Ù…Ù„ Ø¨Ø§ ØªÙ…Ø§Ù… 50+ Ø³ÙˆØ§Ù„
    prompt = f"""You are a workflow analyst. Analyze this referral/excel data and return ONLY JSON with the analysis.

**Input Data:**
{content[:15000]}

**COMPLETE ANALYSIS QUESTIONS (50+ Metrics):**

1. STATUS ANALYSIS (ÙˆØ¶Ø¹ÛŒØª Ø§Ø±Ø¬Ø§Ø¹ÛŒØ§Øª):
   - What percentage of referrals are in "Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø´Ø¯Ù‡" status?
   - Which status has the highest frequency?
   - Average time in "Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø´Ø¯Ù‡" status (based on due date)?
   - Which sender unit has most "Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø´Ø¯Ù‡" referrals?
   - Percentage of "Ø§ØªÙ…Ø§Ù… Ú©Ø§Ø±" referrals vs total?
   - Which receiver has most "Ø¯Ø±Ø­Ø§Ù„ Ù¾ÛŒÚ¯ÛŒØ±ÛŒ" referrals?
   - What is the distribution of all statuses?
   - Which status has the lowest frequency?
   - How many referrals are in "Ù‚Ø¨ÙˆÙ„ Ø§Ø±Ø¬Ø§Ø¹" status?

2. TEMPORAL ANALYSIS (ØªØ­Ù„ÛŒÙ„ Ø²Ù…Ø§Ù†ÛŒ):
   - Which date had most referrals?
   - Average days between registration and due date?
   - Which day was busiest?
   - Percentage of overdue referrals still pending?
   - What is the hourly distribution of referrals?
   - What is the trend between 28th and 29th?
   - Which time of day has most referrals?

3. SUBJECT ANALYSIS (ØªØ­Ù„ÛŒÙ„ Ù…ÙˆØ¶ÙˆØ¹ÛŒ):
   - Most frequent subject/topic?
   - Which subject has most "Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø´Ø¯Ù‡"?
   - Average response time per subject?
   - Which subjects go to "ØªØ¹Ù…ÛŒØ±Ø§Øª" most?
   - Subjects with no descriptions?
   - Second most frequent subject?
   - Which subject has highest completion rate?
   - Which subject has lowest completion rate?
   - List all unique subjects with counts

4. SENDER/RECEIVER ANALYSIS:
   - Top sender by volume?
   - Top receiver by volume?
   - Most common sender-receiver pair?
   - Which receiver has most pending?
   - Which sender has least descriptions?
   - Second top sender?
   - Second top receiver?
   - Which unit collaborates with most others?
   - Sender with highest completion rate?
   - Receiver with highest completion rate?

5. INSTITUTION ANALYSIS:
   - Top institutions by referral count?
   - Most common subject for top institutions?
   - Do higher subscription numbers mean more referrals?
   - Institutions with no descriptions?
   - Which institution has most pending?
   - Which institution has highest completion rate?
   - List all institutions with their subscription codes
   - Correlation between subscription and completion?

6. DESCRIPTION ANALYSIS:
   - Percentage with descriptions?
   - Average description length?
   - Which units write most descriptions?
   - Status of referrals without descriptions?
   - Top keywords in descriptions (like Ø¨Ø§ØªØ±ÛŒ, ÙØ§Ú©ØªÙˆØ±, etc.)?
   - List all unique keywords with frequencies
   - Which keywords correlate with completion?
   - Longest description length?

7. TRACKING ANALYSIS:
   - Which tracking numbers had multiple referrals?
   - Average follow-ups per tracking?
   - Maximum follow-ups for a single tracking?
   - Tracking numbers with most status changes?

8. SUBSCRIPTION ANALYSIS:
   - Highest subscription number?
   - Correlation between subscription and referral count?
   - Average subscription for completed referrals?
   - Average subscription for pending referrals?

9. COMPREHENSIVE INSIGHTS:
   - What factors lead to "Ø§ØªÙ…Ø§Ù… Ú©Ø§Ø±"?
   - Which units collaborate most?
   - Do longer descriptions lead to faster completion?
   - Recurring patterns in referrals?
   - What are the top 3 bottlenecks?
   - What are the top 3 strengths?
   - What are the top 3 risks?
   - Overall health score of the workflow (0-100)?
   - Summary in Persian (minimum 3 sentences)
   - Top 5 recommendations in Persian

Return JSON with this exact structure:
{{
  "status_analysis": {{
    "percent_pending": 25.5,
    "most_frequent_status": "Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø´Ø¯Ù‡",
    "frequent_status_count": 7,
    "avg_days_pending": 2.3,
    "worst_sender_pending": {{"unit": "ØªØ¹Ù…ÛŒØ±Ø§Øª", "count": 3}},
    "percent_completed": 45.8,
    "receiver_with_most_in_progress": {{"receiver": "Ø§Ù…ÙˆØ± Ø®Ø¯Ù…Ø§Øª", "count": 2}},
    "status_distribution": {{
      "Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø´Ø¯Ù‡": 7,
      "Ø±ÙˆÛŒØª Ø´Ø¯Ù‡": 3,
      "Ø¯Ø±Ø­Ø§Ù„ Ù¾ÛŒÚ¯ÛŒØ±ÛŒ": 2,
      "Ø§ØªÙ…Ø§Ù… Ú©Ø§Ø±": 12,
      "Ù‚Ø¨ÙˆÙ„ Ø§Ø±Ø¬Ø§Ø¹": 1
    }},
    "status_with_lowest_frequency": "Ù‚Ø¨ÙˆÙ„ Ø§Ø±Ø¬Ø§Ø¹",
    "lowest_frequency_count": 1
  }},
  
  "temporal_analysis": {{
    "busiest_date": "1404/11/28",
    "daily_counts": {{"1404/11/28": 23, "1404/11/29": 4}},
    "avg_days_to_due": 0,
    "percent_overdue": 0,
    "hourly_distribution": {{"08-10": 15, "10-12": 8, "12-14": 3, "14-16": 1}},
    "trend_description": "Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø§Ø±Ø¬Ø§Ø¹Ø§Øª Ø¯Ø± ØªØ§Ø±ÛŒØ® 28 Ø¨Ù‡Ù…Ù† Ø¨Ø§ 23 Ù…ÙˆØ±Ø¯ Ø«Ø¨Øª Ø´Ø¯Ù‡ Ø§Ø³Øª"
  }},
  
  "subject_analysis": {{
    "most_frequent_subject": "ÙØ§Ú©ØªÙˆØ± Ø´ÙˆØ¯ Ùˆ ØªØ­ÙˆÛŒÙ„",
    "subject_frequency": 6,
    "second_most_frequent": "Ø®Ø±ÛŒØ¯ Ø¨Ø§ØªØ±ÛŒ",
    "second_frequency": 3,
    "subject_pending": {{
      "ÙØ§Ú©ØªÙˆØ± Ø´ÙˆØ¯ Ùˆ ØªØ­ÙˆÛŒÙ„": 2,
      "Ø®Ø±ÛŒØ¯ Ø¨Ø§ØªØ±ÛŒ": 1,
      "Ø§Ø¹Ø²Ø§Ù… Ú©Ø§Ø±Ø´Ù†Ø§Ø³": 1
    }},
    "subject_response_time": {{
      "ÙØ§Ú©ØªÙˆØ± Ø´ÙˆØ¯ Ùˆ ØªØ­ÙˆÛŒÙ„": 1.2,
      "Ø®Ø±ÛŒØ¯ Ø¨Ø§ØªØ±ÛŒ": 2.1,
      "Ø§Ø¹Ø²Ø§Ù… Ú©Ø§Ø±Ø´Ù†Ø§Ø³": 3.5
    }},
    "subject_to_unit": {{
      "Ø®Ø±ÛŒØ¯ Ø¨Ø§ØªØ±ÛŒ": ["Ù¾ÙˆØ±Ø­Ø³ÛŒÙ†", "Ø§Ù…ÙˆØ± Ø®Ø¯Ù…Ø§Øª"],
      "Ø§Ø¹Ø²Ø§Ù… Ú©Ø§Ø±Ø´Ù†Ø§Ø³": ["ØªØ¹Ù…ÛŒØ±Ø§Øª", "Ø§Ù…ÙˆØ± Ø®Ø¯Ù…Ø§Øª"],
      "ÙØ§Ú©ØªÙˆØ± Ø´ÙˆØ¯ Ùˆ ØªØ­ÙˆÛŒÙ„": ["Ú©Ù…Ú©-Ø­Ø³Ø§Ø¨Ø¯Ø§Ø±1", "Ø§Ù…ÙˆØ± Ø®Ø¯Ù…Ø§Øª"]
    }},
    "subjects_no_description": [],
    "unique_subjects": [
      {{"subject": "ÙØ§Ú©ØªÙˆØ± Ø´ÙˆØ¯ Ùˆ ØªØ­ÙˆÛŒÙ„", "count": 6}},
      {{"subject": "Ø®Ø±ÛŒØ¯ Ø¨Ø§ØªØ±ÛŒ", "count": 3}},
      {{"subject": "Ø§Ø¹Ø²Ø§Ù… Ú©Ø§Ø±Ø´Ù†Ø§Ø³", "count": 2}}
    ]
  }},
  
  "sender_receiver_analysis": {{
    "top_senders": [
      {{"sender": "ØªØ¹Ù…ÛŒØ±Ø§Øª", "count": 7, "completion_rate": 57.1}},
      {{"sender": "Ù¾ÙˆØ±Ø­Ø³ÛŒÙ†", "count": 5, "completion_rate": 80.0}},
      {{"sender": "Ø±Ø³ÙˆÙ„ÛŒ", "count": 3, "completion_rate": 66.7}}
    ],
    "top_receivers": [
      {{"receiver": "Ø§Ù…ÙˆØ± Ø®Ø¯Ù…Ø§Øª", "count": 8, "pending": 5}},
      {{"receiver": "Ú©Ù…Ú©-Ø­Ø³Ø§Ø¨Ø¯Ø§Ø±1", "count": 6, "pending": 1}},
      {{"receiver": "Ù¾ÙˆØ±Ø­Ø³ÛŒÙ†", "count": 5, "pending": 1}}
    ],
    "common_pairs": [
      {{"from": "ØªØ¹Ù…ÛŒØ±Ø§Øª", "to": "Ø§Ù…ÙˆØ± Ø®Ø¯Ù…Ø§Øª", "count": 3}},
      {{"from": "Ù¾ÙˆØ±Ø­Ø³ÛŒÙ†", "to": "Ú©Ù…Ú©-Ø­Ø³Ø§Ø¨Ø¯Ø§Ø±1", "count": 2}},
      {{"from": "Ø±Ø³ÙˆÙ„ÛŒ", "to": "Ø§Ù…ÙˆØ± Ø®Ø¯Ù…Ø§Øª", "count": 2}}
    ],
    "receiver_pending": {{"Ø§Ù…ÙˆØ± Ø®Ø¯Ù…Ø§Øª": 5, "ØªØ¹Ù…ÛŒØ±Ø§Øª": 3}},
    "sender_least_description": "Ù…ÙˆÙ…Ù†ÛŒ",
    "most_collaborative_unit": "Ø§Ù…ÙˆØ± Ø®Ø¯Ù…Ø§Øª",
    "collaboration_count": 8
  }},
  
  "institution_analysis": {{
    "top_institutions": [
      {{"name": "Ø³ÛŒÙ…Ø§Ù† Ø¨Ø¬Ù†ÙˆØ±Ø¯", "count": 3, "subs": 28, "completion_rate": 100}},
      {{"name": "Ø¨ÛŒÙ…Ø§Ø±Ø³ØªØ§Ù† Ù†Ù‡Ù… Ø¯ÛŒ ØªØ±Ø¨Øª Ø­ÛŒØ¯Ø±ÛŒÙ‡", "count": 3, "subs": 92, "completion_rate": 100}},
      {{"name": "Ù…ÙˆÙ‚ÙˆÙØ§Øª Ù…Ù„Ú©", "count": 3, "subs": 184, "completion_rate": 0}}
    ],
    "institution_subjects": {{
      "Ø³ÛŒÙ…Ø§Ù† Ø¨Ø¬Ù†ÙˆØ±Ø¯": ["Ø§Ø±Ø³Ø§Ù„ Ø¨Ø§ØªØ±ÛŒ", "ÙØ§Ú©ØªÙˆØ± Ø´ÙˆØ¯ Ùˆ ØªØ­ÙˆÛŒÙ„"],
      "Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø¹Ù„ÙˆÙ… Ù¾Ø²Ø´Ú©ÛŒ Ù…Ø´Ù‡Ø¯": ["ÙØ§Ú©ØªÙˆØ± Ø´ÙˆØ¯ Ùˆ ØªØ­ÙˆÛŒÙ„"],
      "Ø¬Ø§ÛŒÚ¯Ø§Ù‡ Ø³ÙˆØ®Øª Ú©ÙˆÙ‡ Ø³ÙÛŒØ¯": ["Ø§Ø¹Ø²Ø§Ù… Ú©Ø§Ø±Ø´Ù†Ø§Ø³"]
    }},
    "subscription_correlation": 0.3,
    "institutions_no_description": [],
    "institution_with_most_pending": "Ù…ÙˆÙ‚ÙˆÙØ§Øª Ù…Ù„Ú©",
    "pending_count": 3
  }},
  
  "description_analysis": {{
    "percent_with_description": 65.4,
    "avg_description_length": 45.2,
    "max_description_length": 120,
    "top_describers": ["Ù¾ÙˆØ±Ø­Ø³ÛŒÙ†", "Ø±Ø³ÙˆÙ„ÛŒ"],
    "status_without_desc": {{"Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø´Ø¯Ù‡": 3, "Ø±ÙˆÛŒØª Ø´Ø¯Ù‡": 2}},
    "top_keywords": [
      {{"word": "Ø¨Ø§ØªØ±ÛŒ", "count": 6, "completion_rate": 50.0}},
      {{"word": "ÙØ§Ú©ØªÙˆØ±", "count": 5, "completion_rate": 80.0}},
      {{"word": "ØªØ­ÙˆÛŒÙ„", "count": 4, "completion_rate": 75.0}},
      {{"word": "Ø§Ø±Ø³Ø§Ù„", "count": 3, "completion_rate": 66.7}},
      {{"word": "Ú©Ø§Ø±Ø´Ù†Ø§Ø³", "count": 2, "completion_rate": 0.0}}
    ],
    "all_keywords": ["Ø¨Ø§ØªØ±ÛŒ", "ÙØ§Ú©ØªÙˆØ±", "ØªØ­ÙˆÛŒÙ„", "Ø§Ø±Ø³Ø§Ù„", "Ú©Ø§Ø±Ø´Ù†Ø§Ø³", "Ù¾ÛŒØ´â€ŒÙØ§Ú©ØªÙˆØ±", "ØªØ³Øª"]
  }},
  
  "tracking_analysis": {{
    "duplicate_trackings": [
      {{"tracking": 23781, "count": 2, "statuses": ["Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø´Ø¯Ù‡", "Ø±ÙˆÛŒØª Ø´Ø¯Ù‡"]}},
      {{"tracking": 23768, "count": 2, "statuses": ["Ø±ÙˆÛŒØª Ø´Ø¯Ù‡", "Ù‚Ø¨ÙˆÙ„ Ø§Ø±Ø¬Ø§Ø¹"]}},
      {{"tracking": 23766, "count": 2, "statuses": ["Ø§ØªÙ…Ø§Ù… Ú©Ø§Ø±", "Ø§ØªÙ…Ø§Ù… Ú©Ø§Ø±"]}}
    ],
    "avg_followups": 1.2,
    "max_followups": 2,
    "tracking_with_most_changes": 23781
  }},
  
  "subscription_analysis": {{
    "highest_subscription": {{"institution": "Ù…ÙˆÙ‚ÙˆÙØ§Øª Ù…Ù„Ú©", "subs": 184}},
    "lowest_subscription": {{"institution": "Ø³ÛŒÙ…Ø§Ù† Ø¨Ø¬Ù†ÙˆØ±Ø¯", "subs": 28}},
    "avg_subscription_pending": 156,
    "avg_subscription_completed": 89,
    "correlation_coefficient": 0.3,
    "correlation_description": "Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¶Ø¹ÛŒÙ Ø¨ÛŒÙ† Ø§Ø´ØªØ±Ø§Ú© Ùˆ ØªØ¹Ø¯Ø§Ø¯ Ø§Ø±Ø¬Ø§Ø¹"
  }},
  
  "comprehensive_insights": {{
    "completion_factors": [
      "ØªÙˆØ¶ÛŒØ­Ø§Øª Ú©Ø§Ù…Ù„",
      "Ø§Ø±Ø¬Ø§Ø¹ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ù‡ ÙˆØ§Ø­Ø¯ Ù…Ù†Ø§Ø³Ø¨",
      "Ù¾ÛŒÚ¯ÛŒØ±ÛŒ Ù…Ù†Ø¸Ù…",
      "Ø«Ø¨Øª Ø¯Ù‚ÛŒÙ‚ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø´ØªØ±ÛŒ"
    ],
    "top_bottlenecks": [
      {{"bottleneck": "ÙˆØ§Ø­Ø¯ Ø§Ù…ÙˆØ± Ø®Ø¯Ù…Ø§Øª", "pending_count": 5, "impact": "Ø¨Ø§Ù„Ø§"}},
      {{"bottleneck": "ÙˆØ§Ø­Ø¯ ØªØ¹Ù…ÛŒØ±Ø§Øª", "pending_count": 3, "impact": "Ù…ØªÙˆØ³Ø·"}},
      {{"bottleneck": "ÙØ±Ø¢ÛŒÙ†Ø¯ ØªØ§ÛŒÛŒØ¯ ØªØ¹Ù…ÛŒØ±", "pending_count": 2, "impact": "Ù¾Ø§ÛŒÛŒÙ†"}}
    ],
    "top_strengths": [
      "Ù¾ÛŒÚ¯ÛŒØ±ÛŒ Ù…Ù†Ø¸Ù… ØªÙˆØ³Ø· Ù¾ÙˆØ±Ø­Ø³ÛŒÙ†",
      "Ø³Ø±Ø¹Øª Ø¹Ù…Ù„ Ø¯Ø± ÙØ§Ú©ØªÙˆØ±",
      "Ù‡Ù…Ø§Ù‡Ù†Ú¯ÛŒ Ø¨ÛŒÙ† ÙˆØ§Ø­Ø¯Ù‡Ø§"
    ],
    "top_risks": [
      {{"risk": "Ù…Ø´ØªØ±ÛŒ Ø¬Ø§ÛŒÚ¯Ø§Ù‡ Ø³ÙˆØ®Øª Ú©ÙˆÙ‡ Ø³ÙÛŒØ¯", "severity": "Ø¨Ø§Ù„Ø§", "reason": "ØªÚ©Ø±Ø§Ø± Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ø¯ÙˆÙ† Ø±Ø³ÛŒØ¯Ú¯ÛŒ"}},
      {{"risk": "Ù…ÙˆÙ‚ÙˆÙØ§Øª Ù…Ù„Ú©", "severity": "Ù…ØªÙˆØ³Ø·", "reason": "Ø³Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù‡Ù…Ø²Ù…Ø§Ù†"}}
    ],
    "collaborating_units": [
      {{"units": ["ØªØ¹Ù…ÛŒØ±Ø§Øª", "Ø§Ù…ÙˆØ± Ø®Ø¯Ù…Ø§Øª"], "success_rate": 0.8, "collaboration_count": 3}},
      {{"units": ["Ù¾ÙˆØ±Ø­Ø³ÛŒÙ†", "Ú©Ù…Ú©-Ø­Ø³Ø§Ø¨Ø¯Ø§Ø±1"], "success_rate": 1.0, "collaboration_count": 2}}
    ],
    "description_impact": true,
    "description_impact_details": "ØªÙˆØ¶ÛŒØ­Ø§Øª Ú©Ø§Ù…Ù„â€ŒØªØ± Ø¨Ø§Ø¹Ø« Ø§ÙØ²Ø§ÛŒØ´ Û³Û°Ùª Ø¯Ø± Ù†Ø±Ø® ØªÚ©Ù…ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯",
    "recurring_patterns": [
      {{"pattern": "Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ø§ØªØ±ÛŒ", "frequency": 4, "trend": "Ø§ÙØ²Ø§ÛŒØ´ÛŒ"}},
      {{"pattern": "Ø¯Ø±Ø®ÙˆØ§Ø³Øª ÙØ§Ú©ØªÙˆØ±", "frequency": 6, "trend": "Ø«Ø§Ø¨Øª"}},
      {{"pattern": "Ø§Ø¹Ø²Ø§Ù… Ú©Ø§Ø±Ø´Ù†Ø§Ø³", "frequency": 2, "trend": "ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨Ø§ ØªØ§Ø®ÛŒØ±"}}
    ],
    "workflow_health_score": 68.5,
    "health_description": "ÙˆØ¶Ø¹ÛŒØª Ù…ØªÙˆØ³Ø· - Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ø± Ù¾ÛŒÚ¯ÛŒØ±ÛŒ Ùˆ Ù‡Ù…Ø§Ù‡Ù†Ú¯ÛŒ",
    "summary_fa": "Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹ Û²Û· Ø§Ø±Ø¬Ø§Ø¹ØŒ Û±Û² Ù…ÙˆØ±Ø¯ Ø¨Ù‡ Ø§ØªÙ…Ø§Ù… Ø±Ø³ÛŒØ¯Ù‡ (Û´Û´Ùª) Ùˆ Û· Ù…ÙˆØ±Ø¯ Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø´Ø¯Ù‡ (Û²Û¶Ùª). Ú¯Ù„ÙˆÚ¯Ø§Ù‡ Ø§ØµÙ„ÛŒ Ø¯Ø± ÙˆØ§Ø­Ø¯ Ø§Ù…ÙˆØ± Ø®Ø¯Ù…Ø§Øª Ø¨Ø§ Û¸ Ø§Ø±Ø¬Ø§Ø¹ Ø¯Ø±ÛŒØ§ÙØªÛŒ Ùˆ Ûµ Ù…ÙˆØ±Ø¯ Ù…Ø§Ù†Ø¯Ù‡ Ø§Ø³Øª. Ù…Ø´ØªØ±ÛŒØ§Ù† Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒÚ©: Ø³ÛŒÙ…Ø§Ù† Ø¨Ø¬Ù†ÙˆØ±Ø¯ØŒ Ø¨ÛŒÙ…Ø§Ø±Ø³ØªØ§Ù† Ù†Ù‡Ù… Ø¯ÛŒ Ùˆ Ù…ÙˆÙ‚ÙˆÙØ§Øª Ù…Ù„Ú©. Ù‡Ø´Ø¯Ø§Ø±: Ø¬Ø§ÛŒÚ¯Ø§Ù‡ Ø³ÙˆØ®Øª Ú©ÙˆÙ‡ Ø³ÙÛŒØ¯ Ø¨Ø§ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù…Ú©Ø±Ø± Ø§Ø¹Ø²Ø§Ù… Ú©Ø§Ø±Ø´Ù†Ø§Ø³.",
    "recommendations_fa": [
      "Ù¾ÛŒÚ¯ÛŒØ±ÛŒ ÙÙˆØ±ÛŒ Ø§Ø±Ø¬Ø§Ø¹Ø§Øª Ù…Ø¹Ø·Ù„â€ŒÙ…Ø§Ù†Ø¯Ù‡ Ø¯Ø± Ø§Ù…ÙˆØ± Ø®Ø¯Ù…Ø§Øª (Ûµ Ù…ÙˆØ±Ø¯) - Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ§ Û²Û´ Ø³Ø§Ø¹Øª Ø¢ÛŒÙ†Ø¯Ù‡",
      "ØªÙ…Ø§Ø³ Ø¨Ø§ Ø¬Ø§ÛŒÚ¯Ø§Ù‡ Ø³ÙˆØ®Øª Ú©ÙˆÙ‡ Ø³ÙÛŒØ¯ Ùˆ Ø¹Ø°Ø±Ø®ÙˆØ§Ù‡ÛŒ + Ø§Ø¹Ø²Ø§Ù… Ú©Ø§Ø±Ø´Ù†Ø§Ø³ Ø§Ù…Ø±ÙˆØ²",
      "Ø«Ø¨Øª ØªÙˆØ¶ÛŒØ­Ø§Øª Ú©Ø§Ù…Ù„â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø¬Ø§Ø¹Ø§Øª (Û³ÛµÙª Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­ Ù‡Ø³ØªÙ†Ø¯)",
      "Ø¨Ù‡Ø¨ÙˆØ¯ Ù‡Ù…Ø§Ù‡Ù†Ú¯ÛŒ Ø¨ÛŒÙ† ØªØ¹Ù…ÛŒØ±Ø§Øª Ùˆ Ø§Ù…ÙˆØ± Ø®Ø¯Ù…Ø§Øª (Û³ Ø§Ø±Ø¬Ø§Ø¹ Ù…Ø´ØªØ±Ú©)",
      "Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ Ø¬Ù„Ø³Ù‡ Ù‡Ù…Ø§Ù‡Ù†Ú¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø±Ø³ÛŒØ¯Ú¯ÛŒ Ø¨Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ"
    ]
  }}
}}"""
    
    try:
        print(f"ğŸ“¤ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ OpenAI...")
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system", 
                    "content": "You are a workflow analyst specializing in Persian CRM data. Return ONLY valid JSON with no markdown or explanation. Make sure to include ALL fields in the exact structure provided."
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            temperature=0.2,
            max_tokens=8000
        )
        
        response_text = response.choices[0].message.content.strip()
        
        print(f"âœ… Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® - Ø·ÙˆÙ„: {len(response_text)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
        
        # Ø­Ø°Ù markdown Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª
        if response_text.startswith('```'):
            lines = response_text.split('\n')
            json_lines = []
            in_json = False
            for line in lines:
                if line.strip() == '```json' or line.strip() == '```':
                    in_json = not in_json
                    continue
                if in_json or (line.strip().startswith('{') or json_lines):
                    json_lines.append(line)
            response_text = '\n'.join(json_lines).strip()
        
        # Parse JSON
        analysis = json.loads(response_text)
        
        print(f"âœ… JSON Ù¾Ø§Ø±Ø³ Ø´Ø¯")
        print(f"  ÙˆØ¶Ø¹ÛŒØªâ€ŒÙ‡Ø§: {analysis['status_analysis']['status_distribution']}")
        print(f"  Ø§Ù…ØªÛŒØ§Ø² Ø³Ù„Ø§Ù…Øª: {analysis['comprehensive_insights']['workflow_health_score']}")
        print(f"  Ø®Ù„Ø§ØµÙ‡: {analysis['comprehensive_insights']['summary_fa'][:100]}...")
        
        return analysis
        
    except json.JSONDecodeError as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± JSON: {str(e)}")
        print(f"ğŸ“„ Ù…ØªÙ† Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø± (500 Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§ÙˆÙ„): {response_text[:500]}")
        return {
            "error": True,
            "message": "Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"
        }
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            "error": True,
            "message": str(e)
        }


@app.route('/referral')
def referral_page():
    """ØµÙØ­Ù‡ ØªØ­Ù„ÛŒÙ„ Ø§Ø±Ø¬Ø§Ø¹ÛŒØ§Øª"""
    return render_template('referral.html')



@app.route('/api/referral/latest')
def get_latest_referral_analysis():
    """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ø®Ø±ÛŒÙ† ØªØ­Ù„ÛŒÙ„ Ø§Ø±Ø¬Ø§Ø¹ÛŒØ§Øª"""
    try:
        # Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø§ÛŒØ¯ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø®ÙˆÙ†ÛŒØ¯
        # ÙØ¹Ù„Ø§Ù‹ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒÙ…
        
        sample_analysis = {
            "status_analysis": {
                "percent_pending": 25.9,
                "most_frequent_status": "Ø§ØªÙ…Ø§Ù… Ú©Ø§Ø±",
                "frequent_status_count": 12,
                "avg_days_pending": 1.5,
                "worst_sender_pending": {"unit": "ØªØ¹Ù…ÛŒØ±Ø§Øª", "count": 3},
                "percent_completed": 44.4,
                "receiver_with_most_in_progress": {"receiver": "Ø§Ù…ÙˆØ± Ø®Ø¯Ù…Ø§Øª", "count": 2},
                "status_distribution": {
                    "Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø´Ø¯Ù‡": 7,
                    "Ø±ÙˆÛŒØª Ø´Ø¯Ù‡": 3,
                    "Ø¯Ø±Ø­Ø§Ù„ Ù¾ÛŒÚ¯ÛŒØ±ÛŒ": 2,
                    "Ø§ØªÙ…Ø§Ù… Ú©Ø§Ø±": 12,
                    "Ù‚Ø¨ÙˆÙ„ Ø§Ø±Ø¬Ø§Ø¹": 1
                }
            },
            "comprehensive_insights": {
                "summary_fa": "Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹ Û²Û· Ø§Ø±Ø¬Ø§Ø¹ØŒ Û±Û² Ù…ÙˆØ±Ø¯ Ø¨Ù‡ Ø§ØªÙ…Ø§Ù… Ø±Ø³ÛŒØ¯Ù‡ (Û´Û´Ùª) Ùˆ Û· Ù…ÙˆØ±Ø¯ Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø´Ø¯Ù‡ (Û²Û¶Ùª). Ú¯Ù„ÙˆÚ¯Ø§Ù‡ Ø§ØµÙ„ÛŒ Ø¯Ø± ÙˆØ§Ø­Ø¯ Ø§Ù…ÙˆØ± Ø®Ø¯Ù…Ø§Øª Ø¨Ø§ Û¸ Ø§Ø±Ø¬Ø§Ø¹ Ø¯Ø±ÛŒØ§ÙØªÛŒ Ùˆ Ûµ Ù…ÙˆØ±Ø¯ Ù…Ø§Ù†Ø¯Ù‡ Ø§Ø³Øª. Ù…Ø´ØªØ±ÛŒØ§Ù† Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒÚ©: Ø³ÛŒÙ…Ø§Ù† Ø¨Ø¬Ù†ÙˆØ±Ø¯ØŒ Ø¨ÛŒÙ…Ø§Ø±Ø³ØªØ§Ù† Ù†Ù‡Ù… Ø¯ÛŒ Ùˆ Ù…ÙˆÙ‚ÙˆÙØ§Øª Ù…Ù„Ú©. Ù‡Ø´Ø¯Ø§Ø±: Ø¬Ø§ÛŒÚ¯Ø§Ù‡ Ø³ÙˆØ®Øª Ú©ÙˆÙ‡ Ø³ÙÛŒØ¯ Ø¨Ø§ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù…Ú©Ø±Ø± Ø§Ø¹Ø²Ø§Ù… Ú©Ø§Ø±Ø´Ù†Ø§Ø³.",
                "recommendations_fa": [
                    "Ù¾ÛŒÚ¯ÛŒØ±ÛŒ ÙÙˆØ±ÛŒ Ø§Ø±Ø¬Ø§Ø¹Ø§Øª Ù…Ø¹Ø·Ù„â€ŒÙ…Ø§Ù†Ø¯Ù‡ Ø¯Ø± Ø§Ù…ÙˆØ± Ø®Ø¯Ù…Ø§Øª (Ûµ Ù…ÙˆØ±Ø¯)",
                    "ØªÙ…Ø§Ø³ Ø¨Ø§ Ø¬Ø§ÛŒÚ¯Ø§Ù‡ Ø³ÙˆØ®Øª Ú©ÙˆÙ‡ Ø³ÙÛŒØ¯ Ùˆ Ø¹Ø°Ø±Ø®ÙˆØ§Ù‡ÛŒ + Ø§Ø¹Ø²Ø§Ù… Ú©Ø§Ø±Ø´Ù†Ø§Ø³",
                    "Ø«Ø¨Øª ØªÙˆØ¶ÛŒØ­Ø§Øª Ú©Ø§Ù…Ù„â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø¬Ø§Ø¹Ø§Øª (Û³ÛµÙª Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­)",
                    "Ø¨Ù‡Ø¨ÙˆØ¯ Ù‡Ù…Ø§Ù‡Ù†Ú¯ÛŒ Ø¨ÛŒÙ† ØªØ¹Ù…ÛŒØ±Ø§Øª Ùˆ Ø§Ù…ÙˆØ± Ø®Ø¯Ù…Ø§Øª (Û³ Ø§Ø±Ø¬Ø§Ø¹ Ù…Ø´ØªØ±Ú©)"
                ]
            }
        }
        
        return jsonify(sample_analysis)
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§: {str(e)}")
        return jsonify({"error": True, "message": str(e)}), 500


# ========================================
# USERS ANALYSIS ROUTES
# ========================================


@app.route('/api/analysis/latest')
def get_latest_analysis():
    """Ø¢Ø®Ø±ÛŒÙ† ØªØ­Ù„ÛŒÙ„ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡"""
    try:
        conn = get_db_connection()
        if not conn:
            return jsonify({'error': True, 'message': 'Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³'}), 500
            
        cursor = conn.cursor()
        
        # Ø¯Ø±ÛŒØ§ÙØª Ø¢Ø®Ø±ÛŒÙ† ØªØ­Ù„ÛŒÙ„
        cursor.execute('''
            SELECT 
                id, 
                file_name, 
                analyzed_at,
                full_analysis
            FROM analyses 
            ORDER BY analyzed_at DESC 
            LIMIT 1
        ''')
        
        row = cursor.fetchone()
        conn.close()
        
        if not row:
            return jsonify({'error': True, 'message': 'ØªØ­Ù„ÛŒÙ„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯'}), 404
        
        # Parse Ú©Ø±Ø¯Ù† JSON Ú©Ø§Ù…Ù„
        full_analysis = json.loads(row[3]) if row[3] else {}
        
        # Ø³Ø§Ø®Øª response
        analysis = {
            'id': row[0],
            'file_name': row[1],
            'analyzed_at': row[2].isoformat() if row[2] else None,
            'ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ_Ø¹Ø¯Ø¯ÛŒ': full_analysis.get('ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ_Ø¹Ø¯Ø¯ÛŒ', {}),
            'ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ_Ù…ØªÙ†ÛŒ': full_analysis.get('ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ_Ù…ØªÙ†ÛŒ', {}),
            'Ø¢Ù…Ø§Ø±': full_analysis.get('Ø¢Ù…Ø§Ø±', {}),
            'Ø¨Ù‡ØªØ±ÛŒÙ†_Ù‡Ø§': full_analysis.get('Ø¨Ù‡ØªØ±ÛŒÙ†_Ù‡Ø§', {}),
            'Ø¯Ù„Ø§ÛŒÙ„_Ú©Ø§Ù‡Ø´_Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§': full_analysis.get('Ø¯Ù„Ø§ÛŒÙ„_Ú©Ø§Ù‡Ø´_Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§', {}),
            'Ø¯Ù„Ø§ÛŒÙ„_Ú©Ø³Ø¨_Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§': full_analysis.get('Ø¯Ù„Ø§ÛŒÙ„_Ú©Ø³Ø¨_Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§', {}),
            'Ù„ÛŒØ³Øª_Ù‡Ø§': full_analysis.get('Ù„ÛŒØ³Øª_Ù‡Ø§', {})
        }
        
        return jsonify(analysis)
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± get_latest_analysis: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': True, 'message': str(e)}), 500



@app.route('/api/analyze', methods=['POST'])
def analyze():
    """API Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙØ§ÛŒÙ„"""
    file_path = None
    
    try:
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„
        if 'file' not in request.files:
            return jsonify({"error": "ÙØ§ÛŒÙ„ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª"}), 400
        
        file = request.files['file']
        
        if file.filename == '':
            return jsonify({"error": "ÙØ§ÛŒÙ„ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª"}), 400
        
        # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_filename = f"{timestamp}_{file.filename}"
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], safe_filename)
        
        file.save(file_path)  # â¬…ï¸â¬…ï¸â¬…ï¸ Ø§ÛŒÙ† Ø®Ø· Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯!
        
        # Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† Ù…Ø¬Ø¯Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ†
        with open(file_path, 'rb') as f:
            from io import BytesIO
            file_obj = BytesIO(f.read())
            file_obj.filename = file.filename
            
            content = extract_text_from_file(file_obj)
        
        if not content or len(content.strip()) < 50:
            os.remove(file_path)
            return jsonify({"error": "Ù…Ø­ØªÙˆØ§ÛŒ ÙØ§ÛŒÙ„ Ø®Ø§Ù„ÛŒ ÛŒØ§ Ù†Ø§Ù‚Øµ Ø§Ø³Øª"}), 400
        
        # ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ AI
        analysis = analyze_with_ai(content)
        
        if analysis.get('error'):
            os.remove(file_path)
            return jsonify(analysis), 400
        
        analysis['analyzed_at'] = datetime.now().isoformat()
        analysis['file_name'] = file.filename
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        file_info = {
            'name': file.filename,
            'path': file_path,
            'size': os.path.getsize(file_path),
            'type': file.filename.rsplit('.', 1)[1].lower() if '.' in file.filename else 'unknown'
        }
        
        analysis_id = save_analysis_to_db(file_info, analysis)
        
        if analysis_id:
            analysis['id'] = analysis_id
        
        return jsonify(analysis)
        
    except Exception as e:
        # Ù¾Ø±ÛŒÙ†Øª Ø®Ø·Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚
        print(f"\nâŒâŒâŒ Ø®Ø·Ø§ÛŒ CRITICAL: {type(e).__name__}")
        print(f"Ù¾ÛŒØ§Ù…: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # Ø­Ø°Ù ÙØ§ÛŒÙ„ Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§
        if file_path and os.path.exists(file_path):
            os.remove(file_path)
        
        return jsonify({
            "error": True,
            "message": f"{type(e).__name__}: {str(e)}"
        }), 500




@app.route('/api/history')
def api_history():
    """API Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§"""
    analyses = get_all_analyses()
    return jsonify(analyses)

@app.route('/api/analysis/<int:analysis_id>')
def api_analysis_detail(analysis_id):
    """API Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø²Ø¦ÛŒØ§Øª ÛŒÚ© ØªØ­Ù„ÛŒÙ„"""
    analysis = get_analysis_by_id(analysis_id)
    if analysis:
        return jsonify(analysis)
    return jsonify({"error": "ØªØ­Ù„ÛŒÙ„ ÛŒØ§ÙØª Ù†Ø´Ø¯"}), 404

@app.route('/api/file/<int:analysis_id>')
def download_file(analysis_id):
    """Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ÛŒÚ© ØªØ­Ù„ÛŒÙ„"""
    analysis = get_analysis_by_id(analysis_id)
    if analysis and os.path.exists(analysis['file_path']):
        return send_file(
            analysis['file_path'],
            as_attachment=True,
            download_name=analysis['file_name']
        )
    return jsonify({"error": "ÙØ§ÛŒÙ„ ÛŒØ§ÙØª Ù†Ø´Ø¯"}), 404

@app.route('/api/health')
def health():
    """Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª API"""
    api_key = os.getenv('OPENAI_API_KEY')
    db_conn = get_db_connection()
    
    return jsonify({
        "status": "ok",
        "api_configured": bool(api_key and api_key.startswith('sk-')),
        "db_connected": bool(db_conn)
    })

if __name__ == '__main__':
    # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ
    os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
    print(f"âœ… Ù¾ÙˆØ´Ù‡ Ø¢Ù¾Ù„ÙˆØ¯: {app.config['UPLOAD_FOLDER']}")
    
    # Ø¨Ø±Ø±Ø³ÛŒ API Key
    if not os.getenv('OPENAI_API_KEY'):
        print("âš ï¸  Ù‡Ø´Ø¯Ø§Ø±: Ú©Ù„ÛŒØ¯ OpenAI Ø¯Ø± ÙØ§ÛŒÙ„ .env ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª!")
    else:
        print("âœ… OpenAI API Key ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯Ù‡")
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
    conn = get_db_connection()
    if conn:
        print("âœ… Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø§Ø³Øª")
        conn.close()
    else:
        print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³!")
    
    print("\n" + "="*60)
    print("ğŸš€ Ø³Ø±ÙˆØ± Flask Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª")
    print("ğŸ“ Ø¢Ø¯Ø±Ø³: http://127.0.0.1:5001")
    print("="*60 + "\n")
    
    app.run(debug=True, host='0.0.0.0', port=5001)
